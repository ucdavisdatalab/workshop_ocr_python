{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9113daa9",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc8579",
   "metadata": {},
   "source": [
    "OCR With Pytesseract\n",
    "====================\n",
    "\n",
    "## Setup\n",
    "\n",
    "For this workshop, we will be using a sample set of images prepared to demonstrate\n",
    "some key ocr concepts. Download the [zip file]() and extract it to a directory\n",
    "where you are keeping your notes. \n",
    "\n",
    "Start by importing `pandas` and the `pytesseract` package into your python session with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26366cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d5ed14",
   "metadata": {},
   "source": [
    "We can verify that tesseract is also installed, as well as which version is being\n",
    "used with the `get_tesseract_version` function:\n",
    "```\n",
    "pytesseract.get_tesseract_version()\n",
    "```\n",
    "\n",
    "Next, lets create names for some of the prepared images that we will refer \n",
    "to throughout this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c2a1d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_img = './data/alice_start-gutenberg.jpg'\n",
    "fr_img = './data/fr_ocr-wikipedia.png'\n",
    "kor_img = './data/kor_ocr-wikipedia.png'\n",
    "toc_img = './data/alice_toc-gutenberg.jpg'\n",
    "two_column_img = './data/two_column-google.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cfcb41",
   "metadata": {},
   "source": [
    "::: {note} \n",
    "**Supported Image Formats:**\n",
    "\n",
    "pytesseract can operate on any PIL Image, NumPy array or file path of an image\n",
    "than can be processed by Tessseract. Tesseract supports most image formats:\n",
    "png, jpeg, tiff, bmp, gif.\n",
    "\n",
    "Notably, pytesseract, and tesseract, don't work on Pdf files. In order to\n",
    "perform OCR on a pdf file, you must first convert it to a supported image\n",
    "format. \n",
    ":::\n",
    "\n",
    "## Usage\n",
    "\n",
    "In order to maximize the quality of results from OCR with tesseract, its often\n",
    "necessary to customize the behavior of the OCR through parameters. With\n",
    "tesseract, you can specify one or multiple languages you expect in the\n",
    "document, which OCR engine to use, and information about the layout of the text\n",
    "within the document.  \n",
    "\n",
    "Tesseract by default uses its english training data. Tesseract detects\n",
    "characters and then tries to map the detected characters to its closest\n",
    "neighbor. Both of these processes are greatly effected by the assumed language\n",
    "of the text. With tesseract you can specify the language or languages for the\n",
    "OCR engine to use. Tesseract can be configured to use different OCR 'engine\n",
    "modes'. This can be very useful when working with software or on systems that\n",
    "don't support the newest engines or for which computational performance is a\n",
    "limiting factor. In addition, not all languages have training data for each\n",
    "engine mode. Tesseract also supports different behaviors for how it expects the\n",
    "text to be laid out on the page. For example it supports options for if the\n",
    "image is expected to contain just a single character, a single line, multiple\n",
    "columns, and several others.\n",
    "\n",
    "In addition to modifying the behavior of the OCR engine, we can configure the\n",
    "format of the output. Including how much information we want about the\n",
    "extracted text - such as the location on the page and confidence values for the\n",
    "extracted text.\n",
    "\n",
    "### Simplest Usage\n",
    "\n",
    "The simplest way to get the text from an image with pytesseract is with `pytesseract.image_to_string`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b595b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\nDown the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the bank,\\nand of having nothing to do: once or twice she had peeped into the book her\\nsister was reading, but it had no pictures or conversations in it, ‘and what is\\nthe use of a book,’ thought Alice ‘without pictures or conversation?’\\n\\nSo she was considering in her own mind (as well as she could, for the hot\\nday made her feel very sleepy and stupid), whether the pleasure of making a\\ndaisy-chain would be worth the trouble of getting up and picking the daisies,\\nwhen suddenly a White Rabbit with pink eyes ran close by her.\\n\\nThere was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, ‘Oh dear! Oh\\ndear! I shall be late!’ (when she thought it over afterwards, it occurred to\\nher that she ought to have wondered at this, but at the time it all seemed\\nquite natural); but when the Rabbit actually TOOK A WATCH OUT OF\\nITS WAISTCOAT- POCKET, and looked at it, and then hurried on, Alice\\nstarted to her feet, for it flashed across her mind that she had never before\\nseen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and\\nburning with curiosity, she ran across the field after it, and fortunately was\\njust in time to see it pop down a large rabbit-hole under the hedge.\\n\\nIn another moment down went Alice after it, never once considering how\\nin the world she was to get out again.\\n\\nThe rabbit-hole went straight on like a tunnel for some way, and then\\ndipped suddenly down, so suddenly that Alice had not a moment to think\\nabout stopping herself before she found herself falling down a very deep well.\\n\\nEither the well was very deep, or she fell very slowly, for she had plenty\\nof time as she went down to look about her and to wonder what was going\\n\\n13\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.image_to_string(simple_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1fb75",
   "metadata": {},
   "source": [
    "This returns just a string of all the text detected in the image. Notice that\n",
    "the returned text contains alphabetical characters, digits, and escape\n",
    "characters such as `\\n` which represents a newline. The entire text has been\n",
    "concatenated into a single python string, aggregating all the lines, and words\n",
    "detected on the page by tesseract.\n",
    "\n",
    "This is the simplest way to extract the text from an image, when invoked\n",
    "without additional parameters, the `image_to_string` function uses the default\n",
    "usage options of tesseract. \n",
    "\n",
    "### Language Specification\n",
    "\n",
    "By default, tesseract uses its english training data. This can lead to very\n",
    "poor results if there are non english characters in the image. This is\n",
    "especially true if the image contains text that doesn't use a latin alphabet.\n",
    "Lets look at an example using an image containing French text, and another image \n",
    "containing Korean text. As before, we will be invoking the function without \n",
    "any additional parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ca70b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Reconnaissance optique de caractéres\\n\\nLa reconnaissance optique de caractéres (ROC, ou OCR pour l'anglais optical character recognition), ou\\nocérisation, désigne les procédés informatiques pour la traduction d'images de textes imprimés ou\\ndactylographiés en fichiers de texte.\\n\\nUn ordinateur réclame pour 'exécution de cette tache un logiciel d'OCR. Celui-ci permet de récupérer le texte\\ndans l'image d'un texte imprimé et de le sauvegarder dans un fichier pouvant étre exploité dans un traitement\\nde texte pour enrichissement, et stocké dans une base de données ou sur un autre support exploitable par un\\nsystéme informatique.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.image_to_string(fr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1122a8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bet xt ol}\\n\\n‘lvls, $2] 20) sahara\\n\\n‘BB BAt 24\\\\(Optical character recognition; OCR) Ateto| M74L} 7| A= last SLO] Bars o|o|x| A.\\nMAS 85810} 7/717} AS + Qk= VAS Wetst= AOIch.\\n\\n0[0|4| Atos YS + We SM] Mt BSS ARE BS 7st BABS S92] BACs Helse 2zE\\n\\nAOSM Ubos OCRO|A}T SOY, OCRE 2ISAlSOILt 7124] Al2H(machine vision) 2] SP HOFS AlAHE| A\\nch\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.image_to_string(kor_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b4a48",
   "metadata": {},
   "source": [
    "Notice that these results are not ideal. While the French string is quite\n",
    "close, there are a couple of errors with the accents on the characters, and the\n",
    "Korean string is pretty much useless.\n",
    "\n",
    "In order to use ocr on languages other than english we need to download the\n",
    "language's associated training data for tesseract. The `tesseract` package we\n",
    "installed with conda-forge comes with most of the language training data. Data\n",
    "for tesseract can be found at the [tessdata github\n",
    "repository](https://github.com/tesseract-ocr/tessdata). \n",
    "\n",
    "With pytesseract we can see all the available languages with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed1b77a",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afr',\n",
       " 'amh',\n",
       " 'ara',\n",
       " 'asm',\n",
       " 'aze',\n",
       " 'aze_cyrl',\n",
       " 'bel',\n",
       " 'ben',\n",
       " 'bod',\n",
       " 'bos',\n",
       " 'bre',\n",
       " 'bul',\n",
       " 'cat',\n",
       " 'ceb',\n",
       " 'ces',\n",
       " 'chi_sim',\n",
       " 'chi_sim_vert',\n",
       " 'chi_tra',\n",
       " 'chi_tra_vert',\n",
       " 'chr',\n",
       " 'cos',\n",
       " 'cym',\n",
       " 'dan',\n",
       " 'deu',\n",
       " 'div',\n",
       " 'dzo',\n",
       " 'ell',\n",
       " 'eng',\n",
       " 'enm',\n",
       " 'epo',\n",
       " 'equ',\n",
       " 'est',\n",
       " 'eus',\n",
       " 'fao',\n",
       " 'fas',\n",
       " 'fil',\n",
       " 'fin',\n",
       " 'fra',\n",
       " 'frk',\n",
       " 'frm',\n",
       " 'fry',\n",
       " 'gla',\n",
       " 'gle',\n",
       " 'glg',\n",
       " 'grc',\n",
       " 'guj',\n",
       " 'hat',\n",
       " 'heb',\n",
       " 'hin',\n",
       " 'hrv',\n",
       " 'hun',\n",
       " 'hye',\n",
       " 'iku',\n",
       " 'ind',\n",
       " 'isl',\n",
       " 'ita',\n",
       " 'ita_old',\n",
       " 'jav',\n",
       " 'jpn',\n",
       " 'jpn_vert',\n",
       " 'kan',\n",
       " 'kat',\n",
       " 'kat_old',\n",
       " 'kaz',\n",
       " 'khm',\n",
       " 'kir',\n",
       " 'kmr',\n",
       " 'kor',\n",
       " 'kor_vert',\n",
       " 'lao',\n",
       " 'lat',\n",
       " 'lav',\n",
       " 'lit',\n",
       " 'ltz',\n",
       " 'mal',\n",
       " 'mar',\n",
       " 'mkd',\n",
       " 'mlt',\n",
       " 'mon',\n",
       " 'mri',\n",
       " 'msa',\n",
       " 'mya',\n",
       " 'nep',\n",
       " 'nld',\n",
       " 'nor',\n",
       " 'oci',\n",
       " 'ori',\n",
       " 'osd',\n",
       " 'pan',\n",
       " 'pol',\n",
       " 'por',\n",
       " 'pus',\n",
       " 'que',\n",
       " 'ron',\n",
       " 'rus',\n",
       " 'san',\n",
       " 'sin',\n",
       " 'slk',\n",
       " 'slv',\n",
       " 'snd',\n",
       " 'snum',\n",
       " 'spa',\n",
       " 'spa_old',\n",
       " 'sqi',\n",
       " 'srp',\n",
       " 'srp_latn',\n",
       " 'sun',\n",
       " 'swa',\n",
       " 'swe',\n",
       " 'syr',\n",
       " 'tam',\n",
       " 'tat',\n",
       " 'tel',\n",
       " 'tgk',\n",
       " 'tha',\n",
       " 'tir',\n",
       " 'ton',\n",
       " 'tur',\n",
       " 'uig',\n",
       " 'ukr',\n",
       " 'urd',\n",
       " 'uzb',\n",
       " 'uzb_cyrl',\n",
       " 'vie',\n",
       " 'yid',\n",
       " 'yor']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.get_languages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8932416",
   "metadata": {},
   "source": [
    "To specify the language to use, pass the name of the language as a parameter to\n",
    "`pytesseract.image_to_string`. Lets rerun the ocr on the korean image, this\n",
    "time specifying the appropriate language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f6eea53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'광학 문자 인식\\n\\n위키백과, 우리 모두의 백과사전.\\n\\n광학 문자 인식(20068! 08180 『600901007; 0ㄷㅠ83:은 사람이 쓰거나 기계로 인쇄한 문자의 영상을 이미지 스\\n캐너로 획득하여 기계가 읽을 수 있는 문자로 변환하는 것이다.\\n\\n이미지 스캔으로 얻을 수 있는 문서의 활자 영상을 컴퓨터가 편집 가능한 문자코드 등의 형식으로 변환하는 소프트\\n\\n웨어로써 일반적으로 0ㄷ이라고 하며, 0은 인공지능이나 기계 시각(07106 1510/의 연구분야로 시작되었\\n다\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.image_to_string(kor_img, lang='kor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae09426",
   "metadata": {},
   "source": [
    "Tesseract supports images that contain multiple languages, we can specify which\n",
    "languages to use by separating them with the `+` character in the configuration\n",
    "string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6f2f25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'광학 문자 인식\\n\\n위키백과, 우리 모두의 백과사전.\\n\\n광학 문자 인식(20068! character recognition; OCR) 사람이 쓰거나 기계로 인쇄한 문자의 영상을 이미지 스\\n캐너로 획득하여 기계가 읽을 수 있는 문자로 변환하는 것이다.\\n\\n이미지 스캔으로 얻을 수 있는 문서의 활자 영상을 컴퓨터가 편집 가능한 문자코드 등의 형식으로 변환하는 소프트\\n\\n웨어로써 일반적으로 0ㄷ이라고 하며, OCRE 인공지능이나 기계 Al2H(machine 1510/의 연구분야로 시작되었\\n다\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.image_to_string(kor_img, lang='kor+eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a139d8",
   "metadata": {},
   "source": [
    "### Engine Selection\n",
    "\n",
    "Tesseract has several **engine modes** that can be used. There are two main\n",
    "implementations - the original tesseract engine, and, since Tesseract version\n",
    "4, an LSTM based OCR engine. In addition, Tesseract supports using a\n",
    "combination of the two. The list of Tesseract's engine modes:\n",
    "\n",
    "```\n",
    "0 = Original Tesseract only.\n",
    "1 = Neural nets LSTM only.\n",
    "2 = Tesseract + LSTM.\n",
    "3 = Default, based on what is available.\n",
    "```\n",
    "\n",
    "By default Tesseract uses mode 3, which is generally equivalent to option 2.\n",
    "\n",
    "To set the 'oem' (OCR engine mode) with pytesseract we pass it as the 'config' parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67e20e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\nDown the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the bank,\\nand of having nothing to do: once or twice she had peeped into the book her\\nsister was reading, but it had no pictures or conversations in it, ‘and what is\\nthe use of a book,’ thought Alice ‘without pictures or conversation?’\\n\\nSo she was considering in her own mind (as well as she could, for the hot\\nday made her feel very sleepy and stupid), whether the pleasure of making a\\ndaisy-chain would be worth the trouble of getting up and picking the daisies,\\nwhen suddenly a White Rabbit with pink eyes ran close by her.\\n\\nThere was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, ‘Oh dear! Oh\\ndear! I shall be late!’ (when she thought it over afterwards, it occurred to\\nher that she ought to have wondered at this, but at the time it all seemed\\nquite natural); but when the Rabbit actually TOOK A WATCH OUT OF\\nITS WAISTCOAT- POCKET, and looked at it, and then hurried on, Alice\\nstarted to her feet, for it flashed across her mind that she had never before\\nseen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and\\nburning with curiosity, she ran across the field after it, and fortunately was\\njust in time to see it pop down a large rabbit-hole under the hedge.\\n\\nIn another moment down went Alice after it, never once considering how\\nin the world she was to get out again.\\n\\nThe rabbit-hole went straight on like a tunnel for some way, and then\\ndipped suddenly down, so suddenly that Alice had not a moment to think\\nabout stopping herself before she found herself falling down a very deep well.\\n\\nEither the well was very deep, or she fell very slowly, for she had plenty\\nof time as she went down to look about her and to wonder what was going\\n\\n13\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_oem_psm_config = r'--oem 1'\n",
    "pytesseract.image_to_string(simple_img, config=custom_oem_psm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f04d56",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "The `r` before the string in the above code section tells python to treat the\n",
    "string as a sequence of literal characters. This is different behavior from\n",
    "just using a regular python string. In python, strings prefixed with the `r`\n",
    "are called Raw strings.  \n",
    ":::\n",
    "\n",
    "I recommend using option 1 for the best accuracy, unless you are running into\n",
    "specific constraints.  For example, are using an older version of Tesseract\n",
    "(less than version 4, that doesn't have the LSTM option), are running on a\n",
    "system that doesn't support the LSTM (apparently some android builds), or have\n",
    "performance issues.\n",
    "\n",
    "\n",
    "### Page Layouts\n",
    "\n",
    "Tesseract supports a variety of common Page Segmentation Modes.\n",
    "```\n",
    "0 = Orientation and script detection (OSD) only.\n",
    "1 = Automatic page segmentation with OSD.\n",
    "2 = Automatic page segmentation, but no OSD, or OCR. (not implemented)\n",
    "3 = Fully automatic page segmentation, but no OSD. (Default)\n",
    "4 = Assume a single column of text of variable sizes.\n",
    "5 = Assume a single uniform block of vertically aligned text.\n",
    "6 = Assume a single uniform block of text.\n",
    "7 = Treat the image as a single text line.\n",
    "8 = Treat the image as a single word.\n",
    "9 = Treat the image as a single word in a circle.\n",
    "10 = Treat the image as a single character.\n",
    "11 = Sparse text. Find as much text as possible in no particular order.\n",
    "12 = Sparse text with OSD.\n",
    "13 = Raw line. Treat the image as a single text line,\n",
    "     bypassing hacks that are Tesseract-specific.\n",
    "```\n",
    "\n",
    "Just like with the OCR engine mode, we set the Page Segmentation Mode as part\n",
    "of the config string.\n",
    "\n",
    "#### Automatic page segmentation\n",
    "\n",
    "By default, tesseract will attempt to automatically detect the text layout. If\n",
    "we have prior knowledge its best to specify the layout that is most\n",
    "appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34bbfe37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An Overview of the Tesseract OCR Engine\\n\\nRay Smith\\nGoogle Inc.\\ntheraysmith@gmail.com\\n\\nAbstract\\n\\nThe Tesseract OCR engine, as was the HP Research\\nPrototype in the UNLV Fourth Annual Test of OCR\\nAccuracy[1], is described in a _ comprehensive\\noverview. Emphasis is placed on aspects that are novel\\nor at least unusual in an OCR engine, including in\\nparticular the line finding, features/classification\\nmethods, and the adaptive classifier.\\n\\n1. Introduction — Motivation and History\\n\\nTesseract is an open-source OCR engine that was\\ndeveloped at HP between 1984 and 1994. Like a super-\\nnova, it appeared from nowhere for the 1995 UNLV\\nAnnual Test of OCR Accuracy [1], shone brightly with\\nits results, and then vanished back under the same\\ncloak of secrecy under which it had been developed.\\nNow for the first time, details of the architecture and\\nalgorithms can be revealed.\\n\\nTesseract began as a PhD research project [2] in HP\\nLabs, Bristol, and gained momentum as a possible\\nsoftware and/or hardware add-on for HP’s line of\\nflatbed scanners. Motivation was provided by the fact\\nthat the commercial OCR engines of the day were in\\ntheir infancy, and failed miserably on anything but the\\nbest quality print.\\n\\nAfter a joint project between HP Labs Bristol, and\\nHP’s scanner division in Colorado, Tesseract had a\\nsignificant lead in accuracy over the commercial\\nengines, but did not become a product. The next stage\\nof its development was back in HP Labs Bristol as an\\ninvestigation of OCR for compression. Work\\nconcentrated more on improving rejection efficiency\\nthan on base-level accuracy. At the end of this project,\\nat the end of 1994, development ceased entirely. The\\nengine was sent to UNLV for the 1995 Annual Test of\\nOCR Accuracy[1], where it proved its worth against\\nthe commercial engines of the time. In late 2005, HP\\nreleased Tesseract for open source. It is now available\\nat http://code.google.com/p/tesseract-ocr.\\n\\n2. Architecture\\n\\nSince HP had independently-developed page layout\\nanalysis technology that was used in products, (and\\ntherefore not released for open-source) Tesseract never\\nneeded its own page layout analysis. Tesseract\\ntherefore assumes that its input is a binary image with\\noptional polygonal text regions defined.\\n\\nProcessing follows a traditional step-by-step\\npipeline, but some of the stages were unusual in their\\nday, and possibly remain so even now. The first step is\\na connected component analysis in which outlines of\\nthe components are stored. This was a computationally\\nexpensive design decision at the time, but had a\\nsignificant advantage: by inspection of the nesting of\\noutlines, and the number of child and grandchild\\noutlines, it is simple to detect inverse text and\\nrecognize it as easily as black-on-white text. Tesseract\\nwas probably the first OCR engine able to handle\\nwhite-on-black text so trivially. At this stage, outlines\\nare gathered together, purely by nesting, into Blobs.\\n\\nBlobs are organized into text lines, and the lines and\\nregions are analyzed for fixed pitch or proportional\\ntext. Text lines are broken into words differently\\naccording to the kind of character spacing. Fixed pitch\\ntext is chopped immediately by character cells.\\nProportional text is broken into words using definite\\nspaces and fuzzy spaces.\\n\\nRecognition then proceeds as a two-pass process. In\\nthe first pass, an attempt is made to recognize each\\nword in turn. Each word that is satisfactory is passed to\\nan adaptive classifier as training data. The adaptive\\nclassifier then gets a chance to more accurately\\nrecognize text lower down the page.\\n\\nSince the adaptive classifier may have learned\\nsomething useful too late to make a contribution near\\nthe top of the page, a second pass is run over the page,\\nin which words that were not recognized well enough\\nare recognized again.\\n\\nA final phase resolves fuzzy spaces, and checks\\nalternative hypotheses for the x-height to locate small-\\ncap text.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_oem_psm_config = r'--oem 1 --psm 3'\n",
    "pytesseract.image_to_string(two_column_img, config=custom_oem_psm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42934ef7",
   "metadata": {},
   "source": [
    "Notice that with the default page segmentation mode (fully automatic) it \n",
    "correctly identifies that the lines of text are split between the two columns\n",
    "on the page.\n",
    "\n",
    "#### Other psm options\n",
    "\n",
    "Automatic page segmentation might not always be the best option. There are\n",
    "cases when we want to use a different page segmentation mode. One consideration\n",
    "is performance, tesseract will be able to run significantly faster on each\n",
    "image with modes that don't require it to estimate a layout. While this is\n",
    "probably not a big consideration when working with a single image, it adds up\n",
    "when working over hundreds or thousands! Additionally, the automatic page\n",
    "layout detection may give results that don't match your expectations. A common\n",
    "case where its best to explicitly provide a layout option is when working with\n",
    "tabular data.\n",
    "\n",
    "#### Tables\n",
    "\n",
    "Lets look at a common failure of automatic page segmentation. This image\n",
    "contains the table of contents page, where the chapters are aligned to the left\n",
    "and the page numbers to the right. Automatic page segmentation will separate\n",
    "this into two distinct regions, grouping all the left aligned text together and\n",
    "then all of the right aligned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa2beb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contents\\n\\n8\\n\\n9\\n\\nDown the Rabbit-Hole\\n\\nThe Pool of Tears\\n\\nA Caucus-Race and a Long Tale\\nThe Rabbit Sends in a Little Bill\\nAdvice from a Caterpillar\\n\\nPig and Pepper\\n\\nA Mad Tea-Party\\n\\nThe Queen’s Croquet-Ground\\n\\nThe Mock Turtle’s Story\\n\\n10 The Lobster Quadrille\\n\\n11 Who Stole the Tarts?\\n\\n12 Alice’s Evidence\\n\\n11\\n\\n13\\n\\n19\\n\\n25\\n\\n31\\n\\n37\\n\\n43\\n\\n51\\n\\n59\\n\\n67\\n\\n73\\n\\n81\\n\\n87\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_oem_psm_config = r'--oem 1 --psm 3' \n",
    "pytesseract.image_to_string(toc_img, config=custom_oem_psm_config) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0513d",
   "metadata": {},
   "source": [
    "If no segmentation mode exactly matches what you are looking for in terms of\n",
    "grouping together text on the page, you can manually reconstruct the content\n",
    "by using the positional data output by tesseract.\n",
    "\n",
    "\n",
    "### Output Formats\n",
    "\n",
    "So far, we have just been extracting the text as a python string. However,\n",
    "tesseract, and pytesseract, support a variety of output options. Some of these\n",
    "options contain more information than can be stored in just a string. This\n",
    "means we can use that information to get more use out of our ocr results in\n",
    "some cases. Additionally, these output formats can often be interpreted by\n",
    "other software, or may be useful in some bigger pipeline such as a web app.\n",
    "\n",
    "Pytesseract supports the following output formats which can be specified by the\n",
    "function.\n",
    "\n",
    "| Output    | Function               | Return Type    | Description | \n",
    "| --------- | ---------------------- | -----          | ----------- |\n",
    "| string    | `image_to_string`      | str            | the extracted text\n",
    "| osd       | `image_to_osd`         | str            | orientation and script as detected by tesseract\n",
    "| boxes     | `image_to_boxes`       | str            | bounding boxes for each character \n",
    "| data      | `image_to_data`        | str/tsv or df  | tab separated table with boxes, confidences, line numbers\n",
    "| alto xml  | `image_to_alto_xml`    | str/xml        | [ALTO XML](https://en.wikipedia.org/wiki/ALTO_(XML)) - standard for representing OCR and layout data in xml\n",
    "| pdf       | `image_to_pdf_or_hocr` | binary/pdf     | Searchable PDF\n",
    "| hocr      | `image_to_pdf_or_hocr` | str/hocr       | [hOCR](https://en.wikipedia.org/wiki/HOCR) - Another standard for representing OCR data as valid html\n",
    "\n",
    "::: {note}\n",
    "Each of these functions accept the `lang` and `config` parameters we have\n",
    "already seen. Some have additional parameters, such as `image_to_data` which\n",
    "accepts an `output_type` parameter which we will use later. \n",
    "\n",
    "In an interactive python session, we can read documentation for functions with\n",
    "the `help` function. See DataLab's [introductory python reader\n",
    "section](https://ucdavisdatalab.github.io/workshop_python_basics/chapters/01_python-basics.html#getting-help)\n",
    "for more information.  In this case, we can see the parameters for the\n",
    "different pytesseract functions with:\n",
    "\n",
    "```\n",
    "help(pytesseract.image_to_alto_xml)\n",
    "```\n",
    ":::\n",
    "\n",
    "::: {tip}\n",
    "To save these outputs to disk we can use python [file objects](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files).\n",
    "\n",
    "For example when working with pdfs:\n",
    "```\n",
    "pdfdata = pytesseract.image_to_pdf_or_hocr('img.png', extension='pdf')\n",
    "with open('output.pdf', 'w+b') as f:\n",
    "    f.write(pdfdata)\n",
    "```\n",
    "\n",
    "Another example saving the output in ALTO xml format:\n",
    "```\n",
    "xml = pytesseract.image_to_alto_xml('img.png')\n",
    "with open('output.xml', 'w') as f:\n",
    "    f.write(xml)\n",
    "```\n",
    ":::\n",
    "\n",
    "#### Verbose OCR Data\n",
    "\n",
    "Pytesseract's `image_to_data` function provides word level data of the ocr\n",
    "output. Parsing this information can be useful for many types of analyses. By\n",
    "default, the return value is a string containing a table of tab seperated\n",
    "values. However, when [pandas](https://pandas.pydata.org/) is loaded, we can\n",
    "have `image_to_data` return a `pandas` **DataFrame** object by setting the\n",
    "`output_type` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b71aaeac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>page_num</th>\n",
       "      <th>block_num</th>\n",
       "      <th>par_num</th>\n",
       "      <th>line_num</th>\n",
       "      <th>word_num</th>\n",
       "      <th>left</th>\n",
       "      <th>top</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>conf</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2480</td>\n",
       "      <td>3507</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>824</td>\n",
       "      <td>444</td>\n",
       "      <td>74</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>824</td>\n",
       "      <td>444</td>\n",
       "      <td>74</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>824</td>\n",
       "      <td>444</td>\n",
       "      <td>74</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>376</td>\n",
       "      <td>824</td>\n",
       "      <td>444</td>\n",
       "      <td>74</td>\n",
       "      <td>96.769348</td>\n",
       "      <td>Contents</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level  page_num  block_num  par_num  line_num  word_num  left  top  width  \\\n",
       "0      1         1          0        0         0         0     0    0   2480   \n",
       "1      2         1          1        0         0         0   376  824    444   \n",
       "2      3         1          1        1         0         0   376  824    444   \n",
       "3      4         1          1        1         1         0   376  824    444   \n",
       "4      5         1          1        1         1         1   376  824    444   \n",
       "\n",
       "   height       conf      text  \n",
       "0    3507  -1.000000       NaN  \n",
       "1      74  -1.000000       NaN  \n",
       "2      74  -1.000000       NaN  \n",
       "3      74  -1.000000       NaN  \n",
       "4      74  96.769348  Contents  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pytesseract.image_to_data(toc_img, config=custom_oem_psm_config, output_type='data.frame')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8aa8e",
   "metadata": {},
   "source": [
    "::: {note}\n",
    "Pandas Dataframes are incredibly powerful for data analysis. For an introduction\n",
    "to Pandas and dataframes see DataLab's Python Basics [reader](https://ucdavisdatalab.github.io/workshop_python_basics/chapters/02_pandas-basics.html#dataframes).\n",
    ":::\n",
    "\n",
    "Here is a summary of each column in this table. Adapted from [this blog post](https://blog.tomrochette.com/tesseract-tsv-format)\n",
    "\n",
    "| Column    | Description                                                                                |\n",
    "|-----------|--------------------------------------------------------------------------------------------|\n",
    "| level     | 1: page, 2: block, 3: paragraph, 4: line, 5: word                                          |\n",
    "| page_num  | starts at 1, indicates page, only useful for multi-page documents                          |\n",
    "| block_num | starts at 0, pages > blocks > paragraphs > lines > words                                   |\n",
    "| par_num   | starts at 0                                                                                |\n",
    "| line_num  | starts at 0                                                                                |\n",
    "| word_num  | starts at 0                                                                                |\n",
    "| left      | x coordinate in pixels, top left corner of the bounding box, from top left corner of image |\n",
    "| top       | y coordinate in pixels, top left corner of the bounding box, from top left corner of image |\n",
    "| width     | width in pixels of bounding box                                                            |\n",
    "| height    | height in pixels of bounding box                                                           |\n",
    "| conf      | confidence value for the word, 0-100, -1 for any row that isn't a word                     |\n",
    "| text      | detected word, NaN or empty for any row that isn't a word                                  |\n",
    "\n",
    "A **bounding box** refers to a rectangular region within the image. Bounding\n",
    "boxes can be used to represent a page, a block, a paragraph, a line, a word or\n",
    "even a character. \n",
    "\n",
    "Analysis of this data can be very useful for projects that rely on the **layout**\n",
    "of the documents. One use of this data is to quickly classify types of pages\n",
    "within your document set, for example, you could develop heuristics for\n",
    "detecting if a page contains a table of contents and filter those out. You\n",
    "could use this data for extracting Titles or Headers other sequences of text\n",
    "that have differing text heights. Additionally, if you only care about the text\n",
    "within a certain region of the page, for example the main article body of a\n",
    "journal article, you could filter out the rows that aren't within that region. \n",
    "\n",
    "In addition to information about the layout, this table contains the\n",
    "**confidence values** associated with each word of detected text. These scores\n",
    "range from 0-100 and reflect the engine's confidence in the detected word.\n",
    "\n",
    "## Assessing Accuracy\n",
    "\n",
    "OCR is a very challenging problem, and while current tools are very advanced and built using the latest technologies, they are imperfect. \n",
    "\n",
    "### Confidence scores\n",
    "\n",
    "One quantitative way of evaluating the OCR's performance is by analyzing the **confidence values** returned from `pytesseract.image_to_data`. With Pandas we can compute some summary statistics on the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5806a122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    65.000000\n",
       "mean     95.051439\n",
       "std       2.124442\n",
       "min      88.821213\n",
       "25%      93.293381\n",
       "50%      96.125114\n",
       "75%      96.529099\n",
       "max      96.911034\n",
       "Name: conf, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"conf\"].loc[data[\"text\"].notna()].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd40a76",
   "metadata": {},
   "source": [
    "We can also sort the words by their confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4000201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>96.911034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>96.907730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>67</td>\n",
       "      <td>96.851425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>of</td>\n",
       "      <td>96.804993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Story</td>\n",
       "      <td>96.780762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Turtle’s</td>\n",
       "      <td>91.321640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Tea-Party</td>\n",
       "      <td>90.653679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Quadrille</td>\n",
       "      <td>90.321297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Alice’s</td>\n",
       "      <td>89.817886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>73</td>\n",
       "      <td>88.821213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          text       conf\n",
       "8            8  96.911034\n",
       "12           9  96.907730\n",
       "128         67  96.851425\n",
       "23          of  96.804993\n",
       "67       Story  96.780762\n",
       "..         ...        ...\n",
       "66    Turtle’s  91.321640\n",
       "55   Tea-Party  90.653679\n",
       "74   Quadrille  90.321297\n",
       "87     Alice’s  89.817886\n",
       "132         73  88.821213\n",
       "\n",
       "[65 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values('conf', ascending=False)[[\"text\", \"conf\"]].loc[data[\"text\"].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7809106",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26c47440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The               5\n",
       "a                 3\n",
       "11                2\n",
       "A                 2\n",
       "the               2\n",
       "and               2\n",
       "12                1\n",
       "Tarts?            1\n",
       "Stole             1\n",
       "Contents          1\n",
       "Who               1\n",
       "Evidence          1\n",
       "Quadrille         1\n",
       "Lobster           1\n",
       "10                1\n",
       "Story             1\n",
       "Alice’s           1\n",
       "25                1\n",
       "13                1\n",
       "19                1\n",
       "Mock              1\n",
       "31                1\n",
       "37                1\n",
       "43                1\n",
       "51                1\n",
       "59                1\n",
       "67                1\n",
       "73                1\n",
       "81                1\n",
       "Turtle’s          1\n",
       "Tea-Party         1\n",
       "Croquet-Ground    1\n",
       "Queen’s           1\n",
       "9                 1\n",
       "Down              1\n",
       "Rabbit-Hole       1\n",
       "Pool              1\n",
       "of                1\n",
       "Tears             1\n",
       "Caucus-Race       1\n",
       "Long              1\n",
       "Tale              1\n",
       "Rabbit            1\n",
       "Sends             1\n",
       "in                1\n",
       "Little            1\n",
       "Bill              1\n",
       "Advice            1\n",
       "from              1\n",
       "Caterpillar       1\n",
       "Pig               1\n",
       "Pepper            1\n",
       "Mad               1\n",
       "8                 1\n",
       "87                1\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56791cc",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n",
    "\n",
    "### Working with Pdfs\n",
    "\n",
    "### Potential obstacles\n",
    "- image quality too low (300 dpi)\n",
    "- skewed image\n",
    "- text too small\n",
    "- high pass filter before binarization\n",
    "- rotation\n",
    "- borders\n",
    "- extraneous details on page\n",
    "- weird font\n",
    "- noise in paper or image\n",
    "\n",
    "### GUI image editing software\n",
    "\n",
    "### Options for programatically applying these fixes\n",
    "\n",
    "imagemagick\n",
    "\n",
    "opencv python\n",
    "\n",
    "## Text Cleaning\n",
    "\n",
    "## Workflow\n",
    "\n",
    "- iterative\n",
    "- look at data\n",
    "- use quantitative validation metrics\n",
    "- develop preprocessing strategy\n",
    "- develop text cleaning strategy"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "source_map": [
   13,
   18,
   30,
   33,
   43,
   49,
   92,
   94,
   115,
   119,
   121,
   134,
   137,
   143,
   145,
   151,
   153,
   172,
   175,
   221,
   224,
   250,
   253,
   325,
   328,
   377,
   379,
   383,
   385,
   389,
   391
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}