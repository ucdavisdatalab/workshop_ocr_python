{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509677b9",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecf7e81",
   "metadata": {},
   "source": [
    "OCR with pytesseract\n",
    "====================\n",
    "\n",
    "links:\n",
    "- https://cran.r-project.org/web/packages/tesseract/index.html\n",
    "- https://tesseract-ocr.github.io/tessdoc/FAQ.html#running-tesseract\n",
    "- https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html#page-segmentation-method\n",
    "- https://pyimagesearch.com/2021/08/16/installing-tesseract-pytesseract-and-python-ocr-packages-on-your-system/\n",
    "- https://guides.nyu.edu/tesseract/usage\n",
    "- https://github.com/madmaze/pytesseract\n",
    "- https://nanonets.com/blog/ocr-with-tesseract/\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "779abd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Version('5.1.0')>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytesseract\n",
    "\n",
    "pytesseract.get_tesseract_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1831fa81",
   "metadata": {},
   "source": [
    "### Loading the images into python\n",
    "An image file that has been appropriately prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb650c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_img = './data/alice_start-gutenberg.jpg'\n",
    "fr_img = './data/fr_ocr-wikipedia.png'\n",
    "kor_img = './data/kor_ocr-wikipedia.png'\n",
    "toc = './data/alic_toc-gutenberg.jpg'\n",
    "two_column = './data/two_column-google.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d644b23",
   "metadata": {},
   "source": [
    "#### supported image formats\n",
    "\n",
    "pytesseract can operate on any PIL Image, NumPy array or file path of an image\n",
    "than can be processed by Tessseract. Tesseract supports most image formats:\n",
    "png, jpeg, tiff, bmp, gif.\n",
    "\n",
    "Notably, pytesseract, and tesseract, don't work on Pdf files. In order to\n",
    "perform OCR on a pdf file, you must first convert it to a supported image\n",
    "format. See the 'Prework' section for details on how to do this.\n",
    "\n",
    "## Usage\n",
    "\n",
    "In order to maximize the quality of results from OCR with tesseract, its often\n",
    "necessary to customize the behavior of the OCR through parameters. With\n",
    "tesseract, you can specify one or multiple languages you expect in the\n",
    "document, which OCR engine to use, and information about the layout of the text\n",
    "within the document.  \n",
    "\n",
    "Tesseract by default uses its english training data. Tesseract detects\n",
    "characters and then tries to map the detected characters to its closest\n",
    "neighbor. Both of these processes are greatly effected by the assumed language\n",
    "of the text. With tesseract you can specify the language or languages for the\n",
    "OCR engine to use. Tesseract can be configured to use different OCR 'engine\n",
    "modes'. This can be very useful when working with software or on systems that\n",
    "don't support the newest engines or for which computational performance is a\n",
    "limiting factor. In addition, not all languages have training data for each\n",
    "engine mode. Tesseract also supports different behaviors for how it expects the\n",
    "text to be layed out on the page. For example it supports options for if the\n",
    "image is expected to contain just a single character, a single line, multiple\n",
    "columns, and several others.\n",
    "\n",
    "In addition to modifying the behavior of the OCR engine, we can configure the\n",
    "format of the output. Including how much information we want about the\n",
    "extracted text - such as the location on the page and confidence values for the\n",
    "extracted text.\n",
    "\n",
    "### Simplest Usage\n",
    "\n",
    "The simplest way to get the text from an image with pytesseract is with `pytesseract.image_to_string`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da21cc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\nDown the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the bank,\\nand of having nothing to do: once or twice she had peeped into the book her\\nsister was reading, but it had no pictures or conversations in it, ‘and what is\\nthe use of a book,’ thought Alice ‘without pictures or conversation?’\\n\\nSo she was considering in her own mind (as well as she could, for the hot\\nday made her feel very sleepy and stupid), whether the pleasure of making a\\ndaisy-chain would be worth the trouble of getting up and picking the daisies,\\nwhen suddenly a White Rabbit with pink eyes ran close by her.\\n\\nThere was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, ‘Oh dear! Oh\\ndear! I shall be late!’ (when she thought it over afterwards, it occurred to\\nher that she ought to have wondered at this, but at the time it all seemed\\nquite natural); but when the Rabbit actually TOOK A WATCH OUT OF\\nITS WAISTCOAT- POCKET, and looked at it, and then hurried on, Alice\\nstarted to her feet, for it flashed across her mind that she had never before\\nseen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and\\nburning with curiosity, she ran across the field after it, and fortunately was\\njust in time to see it pop down a large rabbit-hole under the hedge.\\n\\nIn another moment down went Alice after it, never once considering how\\nin the world she was to get out again.\\n\\nThe rabbit-hole went straight on like a tunnel for some way, and then\\ndipped suddenly down, so suddenly that Alice had not a moment to think\\nabout stopping herself before she found herself falling down a very deep well.\\n\\nEither the well was very deep, or she fell very slowly, for she had plenty\\nof time as she went down to look about her and to wonder what was going\\n\\n13\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.image_to_string(simple_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af8f98",
   "metadata": {},
   "source": [
    "This returns just a string of all the text detected in the image.\n",
    "\n",
    "### Language Specification\n",
    "\n",
    "By default, tesseract uses its english training data. This can lead to very poor results if there are non english characters in the image. This is especially true if the image contains text that doesn't use a latin alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7380e54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Reconnaissance optique de caractéres\\n\\nLa reconnaissance optique de caractéres (ROC, ou OCR pour l'anglais optical character recognition), ou\\nocérisation, désigne les procédés informatiques pour la traduction d'images de textes imprimés ou\\ndactylographiés en fichiers de texte.\\n\\nUn ordinateur réclame pour 'exécution de cette tache un logiciel d'OCR. Celui-ci permet de récupérer le texte\\ndans l'image d'un texte imprimé et de le sauvegarder dans un fichier pouvant étre exploité dans un traitement\\nde texte pour enrichissement, et stocké dans une base de données ou sur un autre support exploitable par un\\nsystéme informatique.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.image_to_string(fr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95c8bcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bet xt ol}\\n\\n‘lvls, $2] 20) sahara\\n\\n‘BB BAt 24\\\\(Optical character recognition; OCR) Ateto| M74L} 7| A= last SLO] Bars o|o|x| A.\\nMAS 85810} 7/717} AS + Qk= VAS Wetst= AOIch.\\n\\n0[0|4| Atos YS + We SM] Mt BSS ARE BS 7st BABS S92] BACs Helse 2zE\\n\\nAOSM Ubos OCRO|A}T SOY, OCRE 2ISAlSOILt 7124] Al2H(machine vision) 2] SP HOFS AlAHE| A\\nch\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.image_to_string(kor_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cb307e",
   "metadata": {},
   "source": [
    "In order to use ocr on languages other than english we need to download the language's associated training data for tesseract. See index for how to do so on your system.\n",
    "\n",
    "With pytesseract we can see all the available languages with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7e0a5f8",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afr',\n",
       " 'amh',\n",
       " 'ara',\n",
       " 'asm',\n",
       " 'aze',\n",
       " 'aze_cyrl',\n",
       " 'bel',\n",
       " 'ben',\n",
       " 'bod',\n",
       " 'bos',\n",
       " 'bre',\n",
       " 'bul',\n",
       " 'cat',\n",
       " 'ceb',\n",
       " 'ces',\n",
       " 'chi_sim',\n",
       " 'chi_sim_vert',\n",
       " 'chi_tra',\n",
       " 'chi_tra_vert',\n",
       " 'chr',\n",
       " 'cos',\n",
       " 'cym',\n",
       " 'dan',\n",
       " 'deu',\n",
       " 'div',\n",
       " 'dzo',\n",
       " 'ell',\n",
       " 'eng',\n",
       " 'enm',\n",
       " 'epo',\n",
       " 'equ',\n",
       " 'est',\n",
       " 'eus',\n",
       " 'fao',\n",
       " 'fas',\n",
       " 'fil',\n",
       " 'fin',\n",
       " 'fra',\n",
       " 'frk',\n",
       " 'frm',\n",
       " 'fry',\n",
       " 'gla',\n",
       " 'gle',\n",
       " 'glg',\n",
       " 'grc',\n",
       " 'guj',\n",
       " 'hat',\n",
       " 'heb',\n",
       " 'hin',\n",
       " 'hrv',\n",
       " 'hun',\n",
       " 'hye',\n",
       " 'iku',\n",
       " 'ind',\n",
       " 'isl',\n",
       " 'ita',\n",
       " 'ita_old',\n",
       " 'jav',\n",
       " 'jpn',\n",
       " 'jpn_vert',\n",
       " 'kan',\n",
       " 'kat',\n",
       " 'kat_old',\n",
       " 'kaz',\n",
       " 'khm',\n",
       " 'kir',\n",
       " 'kmr',\n",
       " 'kor',\n",
       " 'kor_vert',\n",
       " 'lao',\n",
       " 'lat',\n",
       " 'lav',\n",
       " 'lit',\n",
       " 'ltz',\n",
       " 'mal',\n",
       " 'mar',\n",
       " 'mkd',\n",
       " 'mlt',\n",
       " 'mon',\n",
       " 'mri',\n",
       " 'msa',\n",
       " 'mya',\n",
       " 'nep',\n",
       " 'nld',\n",
       " 'nor',\n",
       " 'oci',\n",
       " 'ori',\n",
       " 'osd',\n",
       " 'pan',\n",
       " 'pol',\n",
       " 'por',\n",
       " 'pus',\n",
       " 'que',\n",
       " 'ron',\n",
       " 'rus',\n",
       " 'san',\n",
       " 'sin',\n",
       " 'slk',\n",
       " 'slv',\n",
       " 'snd',\n",
       " 'snum',\n",
       " 'spa',\n",
       " 'spa_old',\n",
       " 'sqi',\n",
       " 'srp',\n",
       " 'srp_latn',\n",
       " 'sun',\n",
       " 'swa',\n",
       " 'swe',\n",
       " 'syr',\n",
       " 'tam',\n",
       " 'tat',\n",
       " 'tel',\n",
       " 'tgk',\n",
       " 'tha',\n",
       " 'tir',\n",
       " 'ton',\n",
       " 'tur',\n",
       " 'uig',\n",
       " 'ukr',\n",
       " 'urd',\n",
       " 'uzb',\n",
       " 'uzb_cyrl',\n",
       " 'vie',\n",
       " 'yid',\n",
       " 'yor']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.get_languages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c2e9b",
   "metadata": {},
   "source": [
    "To specify the language to use, pass the name of the language as a parameter to `pytesseract.image_to_string`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba66a3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'광학 문자 인식\\n\\n위키백과, 우리 모두의 백과사전.\\n\\n광학 문자 인식(20068! 08180 『600901007; 0ㄷㅠ83:은 사람이 쓰거나 기계로 인쇄한 문자의 영상을 이미지 스\\n캐너로 획득하여 기계가 읽을 수 있는 문자로 변환하는 것이다.\\n\\n이미지 스캔으로 얻을 수 있는 문서의 활자 영상을 컴퓨터가 편집 가능한 문자코드 등의 형식으로 변환하는 소프트\\n\\n웨어로써 일반적으로 0ㄷ이라고 하며, 0은 인공지능이나 기계 시각(07106 1510/의 연구분야로 시작되었\\n다\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.image_to_string(kor_img, lang='kor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ca62f",
   "metadata": {},
   "source": [
    "multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d846aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'광학 문자 인식\\n\\n위키백과, 우리 모두의 백과사전.\\n\\n광학 문자 인식(20068! character recognition; OCR) 사람이 쓰거나 기계로 인쇄한 문자의 영상을 이미지 스\\n캐너로 획득하여 기계가 읽을 수 있는 문자로 변환하는 것이다.\\n\\n이미지 스캔으로 얻을 수 있는 문서의 활자 영상을 컴퓨터가 편집 가능한 문자코드 등의 형식으로 변환하는 소프트\\n\\n웨어로써 일반적으로 0ㄷ이라고 하며, OCRE 인공지능이나 기계 Al2H(machine 1510/의 연구분야로 시작되었\\n다\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.image_to_string(kor_img, lang='kor+eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f2c26",
   "metadata": {},
   "source": [
    "### Engine Selection\n",
    "\n",
    "Tesseract supports the following options for selecting the engine:\n",
    "```\n",
    "0 = Original Tesseract only.\n",
    "1 = Neural nets LSTM only.\n",
    "2 = Tesseract + LSTM.\n",
    "3 = Default, based on what is available.\n",
    "```\n",
    "\n",
    "I recommend using option 1. The default seems to use option 2. \n",
    "\n",
    "To set the 'oem' (OCR engine mode) with pytesseract we pass it as the 'config' parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb65ca36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\nDown the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the bank,\\nand of having nothing to do: once or twice she had peeped into the book her\\nsister was reading, but it had no pictures or conversations in it, ‘and what is\\nthe use of a book,’ thought Alice ‘without pictures or conversation?’\\n\\nSo she was considering in her own mind (as well as she could, for the hot\\nday made her feel very sleepy and stupid), whether the pleasure of making a\\ndaisy-chain would be worth the trouble of getting up and picking the daisies,\\nwhen suddenly a White Rabbit with pink eyes ran close by her.\\n\\nThere was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, ‘Oh dear! Oh\\ndear! I shall be late!’ (when she thought it over afterwards, it occurred to\\nher that she ought to have wondered at this, but at the time it all seemed\\nquite natural); but when the Rabbit actually TOOK A WATCH OUT OF\\nITS WAISTCOAT- POCKET, and looked at it, and then hurried on, Alice\\nstarted to her feet, for it flashed across her mind that she had never before\\nseen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and\\nburning with curiosity, she ran across the field after it, and fortunately was\\njust in time to see it pop down a large rabbit-hole under the hedge.\\n\\nIn another moment down went Alice after it, never once considering how\\nin the world she was to get out again.\\n\\nThe rabbit-hole went straight on like a tunnel for some way, and then\\ndipped suddenly down, so suddenly that Alice had not a moment to think\\nabout stopping herself before she found herself falling down a very deep well.\\n\\nEither the well was very deep, or she fell very slowly, for she had plenty\\nof time as she went down to look about her and to wonder what was going\\n\\n13\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_oem_psm_config = r'--oem 1'\n",
    "pytesseract.image_to_string(simple_img, config=custom_oem_psm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a02673",
   "metadata": {},
   "source": [
    "### Page Layouts\n",
    "\n",
    "Tesseract supports a variety of common Page Segmentation Modes.\n",
    "```\n",
    "0 = Orientation and script detection (OSD) only.\n",
    "1 = Automatic page segmentation with OSD.\n",
    "2 = Automatic page segmentation, but no OSD, or OCR. (not implemented)\n",
    "3 = Fully automatic page segmentation, but no OSD. (Default)\n",
    "4 = Assume a single column of text of variable sizes.\n",
    "5 = Assume a single uniform block of vertically aligned text.\n",
    "6 = Assume a single uniform block of text.\n",
    "7 = Treat the image as a single text line.\n",
    "8 = Treat the image as a single word.\n",
    "9 = Treat the image as a single word in a circle.\n",
    "10 = Treat the image as a single character.\n",
    "11 = Sparse text. Find as much text as possible in no particular order.\n",
    "12 = Sparse text with OSD.\n",
    "13 = Raw line. Treat the image as a single text line,\n",
    "     bypassing hacks that are Tesseract-specific.\n",
    "```\n",
    "\n",
    "Just like with the OCR engine mode, we set the Page Segmentation Mode as part of the config string.\n",
    "\n",
    "#### Auto page segmentation\n",
    "\n",
    "By default, tesseract will attempt to automatically detect the text layout. If\n",
    "we have prior knowledge its best to specify the layout that is most\n",
    "appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fe53f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An Overview of the Tesseract OCR Engine\\n\\nRay Smith\\nGoogle Inc.\\ntheraysmith@gmail.com\\n\\nAbstract\\n\\nThe Tesseract OCR engine, as was the HP Research\\nPrototype in the UNLV Fourth Annual Test of OCR\\nAccuracy[1], is described in a _ comprehensive\\noverview. Emphasis is placed on aspects that are novel\\nor at least unusual in an OCR engine, including in\\nparticular the line finding, features/classification\\nmethods, and the adaptive classifier.\\n\\n1. Introduction — Motivation and History\\n\\nTesseract is an open-source OCR engine that was\\ndeveloped at HP between 1984 and 1994. Like a super-\\nnova, it appeared from nowhere for the 1995 UNLV\\nAnnual Test of OCR Accuracy [1], shone brightly with\\nits results, and then vanished back under the same\\ncloak of secrecy under which it had been developed.\\nNow for the first time, details of the architecture and\\nalgorithms can be revealed.\\n\\nTesseract began as a PhD research project [2] in HP\\nLabs, Bristol, and gained momentum as a possible\\nsoftware and/or hardware add-on for HP’s line of\\nflatbed scanners. Motivation was provided by the fact\\nthat the commercial OCR engines of the day were in\\ntheir infancy, and failed miserably on anything but the\\nbest quality print.\\n\\nAfter a joint project between HP Labs Bristol, and\\nHP’s scanner division in Colorado, Tesseract had a\\nsignificant lead in accuracy over the commercial\\nengines, but did not become a product. The next stage\\nof its development was back in HP Labs Bristol as an\\ninvestigation of OCR for compression. Work\\nconcentrated more on improving rejection efficiency\\nthan on base-level accuracy. At the end of this project,\\nat the end of 1994, development ceased entirely. The\\nengine was sent to UNLV for the 1995 Annual Test of\\nOCR Accuracy[1], where it proved its worth against\\nthe commercial engines of the time. In late 2005, HP\\nreleased Tesseract for open source. It is now available\\nat http://code.google.com/p/tesseract-ocr.\\n\\n2. Architecture\\n\\nSince HP had independently-developed page layout\\nanalysis technology that was used in products, (and\\ntherefore not released for open-source) Tesseract never\\nneeded its own page layout analysis. Tesseract\\ntherefore assumes that its input is a binary image with\\noptional polygonal text regions defined.\\n\\nProcessing follows a traditional step-by-step\\npipeline, but some of the stages were unusual in their\\nday, and possibly remain so even now. The first step is\\na connected component analysis in which outlines of\\nthe components are stored. This was a computationally\\nexpensive design decision at the time, but had a\\nsignificant advantage: by inspection of the nesting of\\noutlines, and the number of child and grandchild\\noutlines, it is simple to detect inverse text and\\nrecognize it as easily as black-on-white text. Tesseract\\nwas probably the first OCR engine able to handle\\nwhite-on-black text so trivially. At this stage, outlines\\nare gathered together, purely by nesting, into Blobs.\\n\\nBlobs are organized into text lines, and the lines and\\nregions are analyzed for fixed pitch or proportional\\ntext. Text lines are broken into words differently\\naccording to the kind of character spacing. Fixed pitch\\ntext is chopped immediately by character cells.\\nProportional text is broken into words using definite\\nspaces and fuzzy spaces.\\n\\nRecognition then proceeds as a two-pass process. In\\nthe first pass, an attempt is made to recognize each\\nword in turn. Each word that is satisfactory is passed to\\nan adaptive classifier as training data. The adaptive\\nclassifier then gets a chance to more accurately\\nrecognize text lower down the page.\\n\\nSince the adaptive classifier may have learned\\nsomething useful too late to make a contribution near\\nthe top of the page, a second pass is run over the page,\\nin which words that were not recognized well enough\\nare recognized again.\\n\\nA final phase resolves fuzzy spaces, and checks\\nalternative hypotheses for the x-height to locate small-\\ncap text.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_oem_psm_config = r'--oem 1 --psm 3'\n",
    "pytesseract.image_to_string(two_column, config=custom_oem_psm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e14a03",
   "metadata": {},
   "source": [
    "Notice that with the default page segmentation mode (fully automatic) it \n",
    "correctly identifies that the lines of text are split between the two columns\n",
    "on the page.\n",
    "\n",
    "If we were to use a psm that \n",
    "\n",
    "\n",
    "#### Uniform block\n",
    "--psm 6\n",
    "\n",
    "#### Tables\n",
    "\n",
    "--psm 4\n",
    "\n",
    "\n",
    "### Output Formats\n",
    "\n",
    "```\n",
    "string\n",
    "boxes\n",
    "data\n",
    "osd\n",
    "alto xml\n",
    "```\n",
    "\n",
    "#### string\n",
    "\n",
    "#### data\n",
    "\n",
    "\n",
    "## Validation\n",
    "\n",
    "### Confidence scores\n",
    "\n",
    "### Summary statistics\n",
    "\n",
    "### Vocabulary"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "source_map": [
   13,
   18,
   35,
   39,
   43,
   49,
   90,
   92,
   100,
   104,
   106,
   111,
   114,
   117,
   119,
   122,
   124,
   139,
   142,
   173,
   176
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}