
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>OCR With Pytesseract &#8212; Optical Character Recognition (OCR) and Working with Messy Text Data</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="References" href="references.html" />
    <link rel="prev" title="Introduction" href="index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/datalab-logo-full-color-rgb-1.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Optical Character Recognition (OCR) and Working with Messy Text Data</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   Introduction
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   OCR With Pytesseract
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Afterword
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/chapters/01_ocr-basics.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/chapters/01_ocr-basics.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/ucdavisdatalab/workshop_ocr_python"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/ucdavisdatalab/workshop_ocr_python/issues/new?title=Issue%20on%20page%20%2Fchapters/01_ocr-basics.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/ucdavisdatalab/workshop_ocr_python/master?urlpath=tree/docs/chapters/01_ocr-basics.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pytesseract-usage">
   Pytesseract Usage
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simplest-usage">
     Simplest Usage
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#language-specification">
     Language Specification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#engine-selection">
     Engine Selection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#page-layouts">
     Page Layouts
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#automatic-page-segmentation">
       Automatic page segmentation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#other-psm-options">
       Other psm options
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tables">
       Tables
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#output-formats">
     Output Formats
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#verbose-ocr-data">
       Verbose OCR Data
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assessing-accuracy">
   Assessing Accuracy
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#confidence-scores">
     Confidence scores
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vocabulary">
     Vocabulary
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-considerations">
   Image Considerations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#working-with-pdfs">
     Working with Pdfs
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#extracting-existing-text-layers">
       extracting existing text layers
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#converting-pdfs-to-an-image-format">
       converting pdfs to an image format
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#text-cleaning">
   Text Cleaning
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>OCR With Pytesseract</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pytesseract-usage">
   Pytesseract Usage
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#simplest-usage">
     Simplest Usage
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#language-specification">
     Language Specification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#engine-selection">
     Engine Selection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#page-layouts">
     Page Layouts
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#automatic-page-segmentation">
       Automatic page segmentation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#other-psm-options">
       Other psm options
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tables">
       Tables
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#output-formats">
     Output Formats
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#verbose-ocr-data">
       Verbose OCR Data
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assessing-accuracy">
   Assessing Accuracy
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#confidence-scores">
     Confidence scores
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vocabulary">
     Vocabulary
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#image-considerations">
   Image Considerations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#working-with-pdfs">
     Working with Pdfs
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#extracting-existing-text-layers">
       extracting existing text layers
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#converting-pdfs-to-an-image-format">
       converting pdfs to an image format
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#text-cleaning">
   Text Cleaning
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="ocr-with-pytesseract">
<h1>OCR With Pytesseract<a class="headerlink" href="#ocr-with-pytesseract" title="Permalink to this headline">¶</a></h1>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<p>For this workshop, we will be using a sample set of images prepared to demonstrate
some key ocr concepts. Download this <a class="reference external" href="https://github.com/ucdavisdatalab/workshop_ocr_python/raw/main/data.zip">zipped folder of images</a> and extract it to a directory
where you are keeping your notes.</p>
<p>Start by importing <code class="docutils literal notranslate"><span class="pre">pandas</span></code> and the <code class="docutils literal notranslate"><span class="pre">pytesseract</span></code> package into your python session with:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">pytesseract</span>
</pre></div>
</div>
</div>
</div>
<p>We can verify that tesseract is also installed, as well as which version is being
used with the <code class="docutils literal notranslate"><span class="pre">get_tesseract_version</span></code> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pytesseract</span><span class="o">.</span><span class="n">get_tesseract_version</span><span class="p">()</span>
</pre></div>
</div>
<p>Next, lets create names for some of the prepared images that we will refer
to throughout this workshop.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">simple_img</span> <span class="o">=</span> <span class="s1">&#39;./data/alice_start-gutenberg.jpg&#39;</span>
<span class="n">fr_img</span> <span class="o">=</span> <span class="s1">&#39;./data/fr_ocr-wikipedia.png&#39;</span>
<span class="n">kor_img</span> <span class="o">=</span> <span class="s1">&#39;./data/kor_ocr-wikipedia.png&#39;</span>
<span class="n">toc_img</span> <span class="o">=</span> <span class="s1">&#39;./data/alice_toc-gutenberg.jpg&#39;</span>
<span class="n">two_column_img</span> <span class="o">=</span> <span class="s1">&#39;./data/two_column-google.png&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Supported Image Formats:</strong></p>
<p>pytesseract can operate on any PIL Image, NumPy array or file path of an image
than can be processed by Tessseract. Tesseract supports most image formats:
png, jpeg, tiff, bmp, gif.</p>
<p>Notably, pytesseract, and tesseract, don’t work on Pdf files. In order to
perform OCR on a pdf file, you must first convert it to a supported image
format.</p>
</div>
</div>
<div class="section" id="pytesseract-usage">
<h2>Pytesseract Usage<a class="headerlink" href="#pytesseract-usage" title="Permalink to this headline">¶</a></h2>
<p>In order to maximize the quality of results from OCR with tesseract, its often
necessary to customize the behavior of the OCR through parameters. With
tesseract, you can specify one or multiple languages you expect in the
document, which OCR engine to use, and information about the layout of the text
within the document.</p>
<p>Tesseract by default uses its english training data. Tesseract detects
characters and then tries to map the detected characters to its closest
neighbor. Both of these processes are greatly effected by the assumed language
of the text. With tesseract you can specify the language or languages for the
OCR engine to use. Tesseract can be configured to use different OCR ‘engine
modes’. This can be very useful when working with software or on systems that
don’t support the newest engines or for which computational performance is a
limiting factor. In addition, not all languages have training data for each
engine mode. Tesseract also supports different behaviors for how it expects the
text to be laid out on the page. For example it supports options for if the
image is expected to contain just a single character, a single line, multiple
columns, and several others.</p>
<p>In addition to modifying the behavior of the OCR engine, we can configure the
format of the output. Including how much information we want about the
extracted text - such as the location on the page and confidence values for the
extracted text.</p>
<div class="section" id="simplest-usage">
<h3>Simplest Usage<a class="headerlink" href="#simplest-usage" title="Permalink to this headline">¶</a></h3>
<p>The simplest way to get the text from an image with pytesseract is with <code class="docutils literal notranslate"><span class="pre">pytesseract.image_to_string</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">simple_img</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Chapter 1\n\nDown the Rabbit-Hole\n\nAlice was beginning to get very tired of sitting by her sister on the bank,\nand of having nothing to do: once or twice she had peeped into the book her\nsister was reading, but it had no pictures or conversations in it, ‘and what is\nthe use of a book,’ thought Alice ‘without pictures or conversation?’\n\nSo she was considering in her own mind (as well as she could, for the hot\nday made her feel very sleepy and stupid), whether the pleasure of making a\ndaisy-chain would be worth the trouble of getting up and picking the daisies,\nwhen suddenly a White Rabbit with pink eyes ran close by her.\n\nThere was nothing so VERY remarkable in that; nor did Alice think it so\nVERY much out of the way to hear the Rabbit say to itself, ‘Oh dear! Oh\ndear! I shall be late!’ (when she thought it over afterwards, it occurred to\nher that she ought to have wondered at this, but at the time it all seemed\nquite natural); but when the Rabbit actually TOOK A WATCH OUT OF\nITS WAISTCOAT- POCKET, and looked at it, and then hurried on, Alice\nstarted to her feet, for it flashed across her mind that she had never before\nseen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and\nburning with curiosity, she ran across the field after it, and fortunately was\njust in time to see it pop down a large rabbit-hole under the hedge.\n\nIn another moment down went Alice after it, never once considering how\nin the world she was to get out again.\n\nThe rabbit-hole went straight on like a tunnel for some way, and then\ndipped suddenly down, so suddenly that Alice had not a moment to think\nabout stopping herself before she found herself falling down a very deep well.\n\nEither the well was very deep, or she fell very slowly, for she had plenty\nof time as she went down to look about her and to wonder what was going\n\n13\n&#39;
</pre></div>
</div>
</div>
</div>
<p>This returns just a string of all the text detected in the image. Notice that
the returned text contains alphabetical characters, digits, and escape
characters such as <code class="docutils literal notranslate"><span class="pre">\n</span></code> which represents a newline. The entire text has been
concatenated into a single python string, aggregating all the lines, and words
detected on the page by tesseract.</p>
<p>This is the simplest way to extract the text from an image, when invoked
without additional parameters, the <code class="docutils literal notranslate"><span class="pre">image_to_string</span></code> function uses the default
usage options of tesseract.</p>
</div>
<div class="section" id="language-specification">
<h3>Language Specification<a class="headerlink" href="#language-specification" title="Permalink to this headline">¶</a></h3>
<p>By default, tesseract uses its english training data. This can lead to very
poor results if there are non english characters in the image. This is
especially true if the image contains text that doesn’t use a latin alphabet.
Lets look at an example using an image containing French text, and another image
containing Korean text. As before, we will be invoking the function without
any additional parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">fr_img</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;Reconnaissance optique de caractéres\n\nLa reconnaissance optique de caractéres (ROC, ou OCR pour l&#39;anglais optical character recognition), ou\nocérisation, désigne les procédés informatiques pour la traduction d&#39;images de textes imprimés ou\ndactylographiés en fichiers de texte.\n\nUn ordinateur réclame pour &#39;exécution de cette tache un logiciel d&#39;OCR. Celui-ci permet de récupérer le texte\ndans l&#39;image d&#39;un texte imprimé et de le sauvegarder dans un fichier pouvant étre exploité dans un traitement\nde texte pour enrichissement, et stocké dans une base de données ou sur un autre support exploitable par un\nsystéme informatique.\n&quot;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">kor_img</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Bet xt ol}\n\n \n\n‘lvls, $2] 20) sahara\n\n‘BB BAt 24\\(Optical character recognition; OCR) Ateto| M74L} 7| A= last SLO] Bars o|o|x| A.\nMAS 85810} 7/717} AS + Qk= VAS Wetst= AOIch.\n\n0[0|4| Atos YS + We SM] Mt BSS ARE BS 7st BABS S92] BACs Helse 2zE\n\nAOSM Ubos OCRO|A}T SOY, OCRE 2ISAlSOILt 7124] Al2H(machine vision) 2] SP HOFS AlAHE| A\nch\n&#39;
</pre></div>
</div>
</div>
</div>
<p>Notice that these results are not ideal. While the French string is quite
close, there are a couple of errors with the accents on the characters, and the
Korean string is pretty much useless.</p>
<p>In order to use ocr on languages other than english we need to download the
language’s associated training data for tesseract. The <code class="docutils literal notranslate"><span class="pre">tesseract</span></code> package we
installed with conda-forge comes with most of the language training data. Data
for tesseract can be found at the <a class="reference external" href="https://github.com/tesseract-ocr/tessdata">tessdata github
repository</a>.</p>
<p>With pytesseract we can see all the available languages with:</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pytesseract</span><span class="o">.</span><span class="n">get_languages</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;afr&#39;,
 &#39;amh&#39;,
 &#39;ara&#39;,
 &#39;asm&#39;,
 &#39;aze&#39;,
 &#39;aze_cyrl&#39;,
 &#39;bel&#39;,
 &#39;ben&#39;,
 &#39;bod&#39;,
 &#39;bos&#39;,
 &#39;bre&#39;,
 &#39;bul&#39;,
 &#39;cat&#39;,
 &#39;ceb&#39;,
 &#39;ces&#39;,
 &#39;chi_sim&#39;,
 &#39;chi_sim_vert&#39;,
 &#39;chi_tra&#39;,
 &#39;chi_tra_vert&#39;,
 &#39;chr&#39;,
 &#39;cos&#39;,
 &#39;cym&#39;,
 &#39;dan&#39;,
 &#39;deu&#39;,
 &#39;div&#39;,
 &#39;dzo&#39;,
 &#39;ell&#39;,
 &#39;eng&#39;,
 &#39;enm&#39;,
 &#39;epo&#39;,
 &#39;equ&#39;,
 &#39;est&#39;,
 &#39;eus&#39;,
 &#39;fao&#39;,
 &#39;fas&#39;,
 &#39;fil&#39;,
 &#39;fin&#39;,
 &#39;fra&#39;,
 &#39;frk&#39;,
 &#39;frm&#39;,
 &#39;fry&#39;,
 &#39;gla&#39;,
 &#39;gle&#39;,
 &#39;glg&#39;,
 &#39;grc&#39;,
 &#39;guj&#39;,
 &#39;hat&#39;,
 &#39;heb&#39;,
 &#39;hin&#39;,
 &#39;hrv&#39;,
 &#39;hun&#39;,
 &#39;hye&#39;,
 &#39;iku&#39;,
 &#39;ind&#39;,
 &#39;isl&#39;,
 &#39;ita&#39;,
 &#39;ita_old&#39;,
 &#39;jav&#39;,
 &#39;jpn&#39;,
 &#39;jpn_vert&#39;,
 &#39;kan&#39;,
 &#39;kat&#39;,
 &#39;kat_old&#39;,
 &#39;kaz&#39;,
 &#39;khm&#39;,
 &#39;kir&#39;,
 &#39;kmr&#39;,
 &#39;kor&#39;,
 &#39;kor_vert&#39;,
 &#39;lao&#39;,
 &#39;lat&#39;,
 &#39;lav&#39;,
 &#39;lit&#39;,
 &#39;ltz&#39;,
 &#39;mal&#39;,
 &#39;mar&#39;,
 &#39;mkd&#39;,
 &#39;mlt&#39;,
 &#39;mon&#39;,
 &#39;mri&#39;,
 &#39;msa&#39;,
 &#39;mya&#39;,
 &#39;nep&#39;,
 &#39;nld&#39;,
 &#39;nor&#39;,
 &#39;oci&#39;,
 &#39;ori&#39;,
 &#39;osd&#39;,
 &#39;pan&#39;,
 &#39;pol&#39;,
 &#39;por&#39;,
 &#39;pus&#39;,
 &#39;que&#39;,
 &#39;ron&#39;,
 &#39;rus&#39;,
 &#39;san&#39;,
 &#39;sin&#39;,
 &#39;slk&#39;,
 &#39;slv&#39;,
 &#39;snd&#39;,
 &#39;spa&#39;,
 &#39;spa_old&#39;,
 &#39;sqi&#39;,
 &#39;srp&#39;,
 &#39;srp_latn&#39;,
 &#39;sun&#39;,
 &#39;swa&#39;,
 &#39;swe&#39;,
 &#39;syr&#39;,
 &#39;tam&#39;,
 &#39;tat&#39;,
 &#39;tel&#39;,
 &#39;tgk&#39;,
 &#39;tha&#39;,
 &#39;tir&#39;,
 &#39;ton&#39;,
 &#39;tur&#39;,
 &#39;uig&#39;,
 &#39;ukr&#39;,
 &#39;urd&#39;,
 &#39;uzb&#39;,
 &#39;uzb_cyrl&#39;,
 &#39;vie&#39;,
 &#39;yid&#39;,
 &#39;yor&#39;]
</pre></div>
</div>
</div>
</div>
<p>To specify the language to use, pass the name of the language as a parameter to
<code class="docutils literal notranslate"><span class="pre">pytesseract.image_to_string</span></code>. Lets rerun the ocr on the korean image, this
time specifying the appropriate language.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">kor_img</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;kor&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;광학 문자 인식\n\n \n\n위키백과, 우리 모두의 백과사전.\n\n광학 문자 인식(20068! 08180 『600901007; 0ㄷㅠ83:은 사람이 쓰거나 기계로 인쇄한 문자의 영상을 이미지 스\n캐너로 획득하여 기계가 읽을 수 있는 문자로 변환하는 것이다.\n\n이미지 스캔으로 얻을 수 있는 문서의 활자 영상을 컴퓨터가 편집 가능한 문자코드 등의 형식으로 변환하는 소프트\n\n웨어로써 일반적으로 0ㄷ이라고 하며, 0은 인공지능이나 기계 시각(07106 1510/의 연구분야로 시작되었\n다\n&#39;
</pre></div>
</div>
</div>
</div>
<p>Tesseract supports images that contain multiple languages, we can specify which
languages to use by separating them with the <code class="docutils literal notranslate"><span class="pre">+</span></code> character in the configuration
string:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">kor_img</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;kor+eng&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;광학 문자 인식\n\n \n\n위키백과, 우리 모두의 백과사전.\n\n광학 문자 인식(20068! character recognition; OCR) 사람이 쓰거나 기계로 인쇄한 문자의 영상을 이미지 스\n캐너로 획득하여 기계가 읽을 수 있는 문자로 변환하는 것이다.\n\n이미지 스캔으로 얻을 수 있는 문서의 활자 영상을 컴퓨터가 편집 가능한 문자코드 등의 형식으로 변환하는 소프트\n\n웨어로써 일반적으로 0ㄷ이라고 하며, OCRE 인공지능이나 기계 Al2H(machine 1510/의 연구분야로 시작되었\n다\n&#39;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="engine-selection">
<h3>Engine Selection<a class="headerlink" href="#engine-selection" title="Permalink to this headline">¶</a></h3>
<p>Tesseract has several <strong>engine modes</strong> that can be used. There are two main
implementations - the original tesseract engine, and, since Tesseract version
4, an LSTM based OCR engine. In addition, Tesseract supports using a
combination of the two. The list of Tesseract’s engine modes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span> <span class="o">=</span> <span class="n">Original</span> <span class="n">Tesseract</span> <span class="n">only</span><span class="o">.</span>
<span class="mi">1</span> <span class="o">=</span> <span class="n">Neural</span> <span class="n">nets</span> <span class="n">LSTM</span> <span class="n">only</span><span class="o">.</span>
<span class="mi">2</span> <span class="o">=</span> <span class="n">Tesseract</span> <span class="o">+</span> <span class="n">LSTM</span><span class="o">.</span>
<span class="mi">3</span> <span class="o">=</span> <span class="n">Default</span><span class="p">,</span> <span class="n">based</span> <span class="n">on</span> <span class="n">what</span> <span class="ow">is</span> <span class="n">available</span><span class="o">.</span>
</pre></div>
</div>
<p>By default Tesseract uses mode 3, which is generally equivalent to option 2.</p>
<p>To set the ‘oem’ (OCR engine mode) with pytesseract we pass it as the ‘config’ parameter:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">custom_oem_psm_config</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;--oem 1&#39;</span>
<span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">simple_img</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">custom_oem_psm_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Chapter 1\n\nDown the Rabbit-Hole\n\nAlice was beginning to get very tired of sitting by her sister on the bank,\nand of having nothing to do: once or twice she had peeped into the book her\nsister was reading, but it had no pictures or conversations in it, ‘and what is\nthe use of a book,’ thought Alice ‘without pictures or conversation?’\n\nSo she was considering in her own mind (as well as she could, for the hot\nday made her feel very sleepy and stupid), whether the pleasure of making a\ndaisy-chain would be worth the trouble of getting up and picking the daisies,\nwhen suddenly a White Rabbit with pink eyes ran close by her.\n\nThere was nothing so VERY remarkable in that; nor did Alice think it so\nVERY much out of the way to hear the Rabbit say to itself, ‘Oh dear! Oh\ndear! I shall be late!’ (when she thought it over afterwards, it occurred to\nher that she ought to have wondered at this, but at the time it all seemed\nquite natural); but when the Rabbit actually TOOK A WATCH OUT OF\nITS WAISTCOAT- POCKET, and looked at it, and then hurried on, Alice\nstarted to her feet, for it flashed across her mind that she had never before\nseen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and\nburning with curiosity, she ran across the field after it, and fortunately was\njust in time to see it pop down a large rabbit-hole under the hedge.\n\nIn another moment down went Alice after it, never once considering how\nin the world she was to get out again.\n\nThe rabbit-hole went straight on like a tunnel for some way, and then\ndipped suddenly down, so suddenly that Alice had not a moment to think\nabout stopping herself before she found herself falling down a very deep well.\n\nEither the well was very deep, or she fell very slowly, for she had plenty\nof time as she went down to look about her and to wonder what was going\n\n13\n&#39;
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">r</span></code> before the string in the above code section tells python to treat the
string as a sequence of literal characters. This is different behavior from
just using a regular python string. In python, strings prefixed with the <code class="docutils literal notranslate"><span class="pre">r</span></code>
are called Raw strings.</p>
</div>
<p>I recommend using option 1 for the best accuracy, unless you are running into
specific constraints.  For example, are using an older version of Tesseract
(less than version 4, that doesn’t have the LSTM option), are running on a
system that doesn’t support the LSTM (apparently some android builds), or have
performance issues.</p>
</div>
<div class="section" id="page-layouts">
<h3>Page Layouts<a class="headerlink" href="#page-layouts" title="Permalink to this headline">¶</a></h3>
<p>Tesseract supports a variety of common Page Segmentation Modes.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span> <span class="o">=</span> <span class="n">Orientation</span> <span class="ow">and</span> <span class="n">script</span> <span class="n">detection</span> <span class="p">(</span><span class="n">OSD</span><span class="p">)</span> <span class="n">only</span><span class="o">.</span>
<span class="mi">1</span> <span class="o">=</span> <span class="n">Automatic</span> <span class="n">page</span> <span class="n">segmentation</span> <span class="k">with</span> <span class="n">OSD</span><span class="o">.</span>
<span class="mi">2</span> <span class="o">=</span> <span class="n">Automatic</span> <span class="n">page</span> <span class="n">segmentation</span><span class="p">,</span> <span class="n">but</span> <span class="n">no</span> <span class="n">OSD</span><span class="p">,</span> <span class="ow">or</span> <span class="n">OCR</span><span class="o">.</span> <span class="p">(</span><span class="ow">not</span> <span class="n">implemented</span><span class="p">)</span>
<span class="mi">3</span> <span class="o">=</span> <span class="n">Fully</span> <span class="n">automatic</span> <span class="n">page</span> <span class="n">segmentation</span><span class="p">,</span> <span class="n">but</span> <span class="n">no</span> <span class="n">OSD</span><span class="o">.</span> <span class="p">(</span><span class="n">Default</span><span class="p">)</span>
<span class="mi">4</span> <span class="o">=</span> <span class="n">Assume</span> <span class="n">a</span> <span class="n">single</span> <span class="n">column</span> <span class="n">of</span> <span class="n">text</span> <span class="n">of</span> <span class="n">variable</span> <span class="n">sizes</span><span class="o">.</span>
<span class="mi">5</span> <span class="o">=</span> <span class="n">Assume</span> <span class="n">a</span> <span class="n">single</span> <span class="n">uniform</span> <span class="n">block</span> <span class="n">of</span> <span class="n">vertically</span> <span class="n">aligned</span> <span class="n">text</span><span class="o">.</span>
<span class="mi">6</span> <span class="o">=</span> <span class="n">Assume</span> <span class="n">a</span> <span class="n">single</span> <span class="n">uniform</span> <span class="n">block</span> <span class="n">of</span> <span class="n">text</span><span class="o">.</span>
<span class="mi">7</span> <span class="o">=</span> <span class="n">Treat</span> <span class="n">the</span> <span class="n">image</span> <span class="k">as</span> <span class="n">a</span> <span class="n">single</span> <span class="n">text</span> <span class="n">line</span><span class="o">.</span>
<span class="mi">8</span> <span class="o">=</span> <span class="n">Treat</span> <span class="n">the</span> <span class="n">image</span> <span class="k">as</span> <span class="n">a</span> <span class="n">single</span> <span class="n">word</span><span class="o">.</span>
<span class="mi">9</span> <span class="o">=</span> <span class="n">Treat</span> <span class="n">the</span> <span class="n">image</span> <span class="k">as</span> <span class="n">a</span> <span class="n">single</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">circle</span><span class="o">.</span>
<span class="mi">10</span> <span class="o">=</span> <span class="n">Treat</span> <span class="n">the</span> <span class="n">image</span> <span class="k">as</span> <span class="n">a</span> <span class="n">single</span> <span class="n">character</span><span class="o">.</span>
<span class="mi">11</span> <span class="o">=</span> <span class="n">Sparse</span> <span class="n">text</span><span class="o">.</span> <span class="n">Find</span> <span class="k">as</span> <span class="n">much</span> <span class="n">text</span> <span class="k">as</span> <span class="n">possible</span> <span class="ow">in</span> <span class="n">no</span> <span class="n">particular</span> <span class="n">order</span><span class="o">.</span>
<span class="mi">12</span> <span class="o">=</span> <span class="n">Sparse</span> <span class="n">text</span> <span class="k">with</span> <span class="n">OSD</span><span class="o">.</span>
<span class="mi">13</span> <span class="o">=</span> <span class="n">Raw</span> <span class="n">line</span><span class="o">.</span> <span class="n">Treat</span> <span class="n">the</span> <span class="n">image</span> <span class="k">as</span> <span class="n">a</span> <span class="n">single</span> <span class="n">text</span> <span class="n">line</span><span class="p">,</span>
     <span class="n">bypassing</span> <span class="n">hacks</span> <span class="n">that</span> <span class="n">are</span> <span class="n">Tesseract</span><span class="o">-</span><span class="n">specific</span><span class="o">.</span>
</pre></div>
</div>
<p>Just like with the OCR engine mode, we set the Page Segmentation Mode as part
of the config string.</p>
<div class="section" id="automatic-page-segmentation">
<h4>Automatic page segmentation<a class="headerlink" href="#automatic-page-segmentation" title="Permalink to this headline">¶</a></h4>
<p>By default, tesseract will attempt to automatically detect the text layout. If
we have prior knowledge its best to specify the layout that is most
appropriate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">custom_oem_psm_config</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;--oem 1 --psm 3&#39;</span>
<span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">two_column_img</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">custom_oem_psm_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;An Overview of the Tesseract OCR Engine\n\nRay Smith\nGoogle Inc.\ntheraysmith@gmail.com\n\nAbstract\n\nThe Tesseract OCR engine, as was the HP Research\nPrototype in the UNLV Fourth Annual Test of OCR\nAccuracy[1], is described in a _ comprehensive\noverview. Emphasis is placed on aspects that are novel\nor at least unusual in an OCR engine, including in\nparticular the line finding, features/classification\nmethods, and the adaptive classifier.\n\n1. Introduction — Motivation and History\n\nTesseract is an open-source OCR engine that was\ndeveloped at HP between 1984 and 1994. Like a super-\nnova, it appeared from nowhere for the 1995 UNLV\nAnnual Test of OCR Accuracy [1], shone brightly with\nits results, and then vanished back under the same\ncloak of secrecy under which it had been developed.\nNow for the first time, details of the architecture and\nalgorithms can be revealed.\n\nTesseract began as a PhD research project [2] in HP\nLabs, Bristol, and gained momentum as a possible\nsoftware and/or hardware add-on for HP’s line of\nflatbed scanners. Motivation was provided by the fact\nthat the commercial OCR engines of the day were in\ntheir infancy, and failed miserably on anything but the\nbest quality print.\n\nAfter a joint project between HP Labs Bristol, and\nHP’s scanner division in Colorado, Tesseract had a\nsignificant lead in accuracy over the commercial\nengines, but did not become a product. The next stage\nof its development was back in HP Labs Bristol as an\ninvestigation of OCR for compression. Work\nconcentrated more on improving rejection efficiency\nthan on base-level accuracy. At the end of this project,\nat the end of 1994, development ceased entirely. The\nengine was sent to UNLV for the 1995 Annual Test of\nOCR Accuracy[1], where it proved its worth against\nthe commercial engines of the time. In late 2005, HP\nreleased Tesseract for open source. It is now available\nat http://code.google.com/p/tesseract-ocr.\n\n2. Architecture\n\nSince HP had independently-developed page layout\nanalysis technology that was used in products, (and\ntherefore not released for open-source) Tesseract never\nneeded its own page layout analysis. Tesseract\ntherefore assumes that its input is a binary image with\noptional polygonal text regions defined.\n\nProcessing follows a traditional step-by-step\npipeline, but some of the stages were unusual in their\nday, and possibly remain so even now. The first step is\na connected component analysis in which outlines of\nthe components are stored. This was a computationally\nexpensive design decision at the time, but had a\nsignificant advantage: by inspection of the nesting of\noutlines, and the number of child and grandchild\noutlines, it is simple to detect inverse text and\nrecognize it as easily as black-on-white text. Tesseract\nwas probably the first OCR engine able to handle\nwhite-on-black text so trivially. At this stage, outlines\nare gathered together, purely by nesting, into Blobs.\n\nBlobs are organized into text lines, and the lines and\nregions are analyzed for fixed pitch or proportional\ntext. Text lines are broken into words differently\naccording to the kind of character spacing. Fixed pitch\ntext is chopped immediately by character cells.\nProportional text is broken into words using definite\nspaces and fuzzy spaces.\n\nRecognition then proceeds as a two-pass process. In\nthe first pass, an attempt is made to recognize each\nword in turn. Each word that is satisfactory is passed to\nan adaptive classifier as training data. The adaptive\nclassifier then gets a chance to more accurately\nrecognize text lower down the page.\n\nSince the adaptive classifier may have learned\nsomething useful too late to make a contribution near\nthe top of the page, a second pass is run over the page,\nin which words that were not recognized well enough\nare recognized again.\n\nA final phase resolves fuzzy spaces, and checks\nalternative hypotheses for the x-height to locate small-\ncap text.\n&#39;
</pre></div>
</div>
</div>
</div>
<p>Notice that with the default page segmentation mode (fully automatic) it
correctly identifies that the lines of text are split between the two columns
on the page.</p>
</div>
<div class="section" id="other-psm-options">
<h4>Other psm options<a class="headerlink" href="#other-psm-options" title="Permalink to this headline">¶</a></h4>
<p>Automatic page segmentation might not always be the best option. There are
cases when we want to use a different page segmentation mode. One consideration
is performance, tesseract will be able to run significantly faster on each
image with modes that don’t require it to estimate a layout. While this is
probably not a big consideration when working with a single image, it adds up
when working over hundreds or thousands! Additionally, the automatic page
layout detection may give results that don’t match your expectations. A common
case where its best to explicitly provide a layout option is when working with
tabular data.</p>
</div>
<div class="section" id="tables">
<h4>Tables<a class="headerlink" href="#tables" title="Permalink to this headline">¶</a></h4>
<p>Lets look at a common failure of automatic page segmentation. This image
contains the table of contents page, where the chapters are aligned to the left
and the page numbers to the right. Automatic page segmentation will separate
this into two distinct regions, grouping all the left aligned text together and
then all of the right aligned text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">custom_oem_psm_config</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;--oem 1 --psm 3&#39;</span> 
<span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_string</span><span class="p">(</span><span class="n">toc_img</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">custom_oem_psm_config</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Contents\n\n8\n\n9\n\nDown the Rabbit-Hole\n\nThe Pool of Tears\n\nA Caucus-Race and a Long Tale\nThe Rabbit Sends in a Little Bill\nAdvice from a Caterpillar\n\nPig and Pepper\n\nA Mad Tea-Party\n\nThe Queen’s Croquet-Ground\n\nThe Mock Turtle’s Story\n\n10 The Lobster Quadrille\n\n11 Who Stole the Tarts?\n\n12 Alice’s Evidence\n\n11\n\n13\n\n19\n\n25\n\n31\n\n37\n\n43\n\n51\n\n59\n\n67\n\n73\n\n81\n\n87\n&#39;
</pre></div>
</div>
</div>
</div>
<p>If no segmentation mode exactly matches what you are looking for in terms of
grouping together text on the page, you can manually reconstruct the content
by using the positional data output by tesseract.</p>
</div>
</div>
<div class="section" id="output-formats">
<h3>Output Formats<a class="headerlink" href="#output-formats" title="Permalink to this headline">¶</a></h3>
<p>So far, we have just been extracting the text as a python string. However,
tesseract, and pytesseract, support a variety of output options. Some of these
options contain more information than can be stored in just a string. This
means we can use that information to get more use out of our ocr results in
some cases. Additionally, these output formats can often be interpreted by
other software, or may be useful in some bigger pipeline such as a web app.</p>
<p>Pytesseract supports the following output formats which can be specified by the
function.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Output</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Return Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>string</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">image_to_string</span></code></p></td>
<td><p>str</p></td>
<td><p>the extracted text</p></td>
</tr>
<tr class="row-odd"><td><p>osd</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">image_to_osd</span></code></p></td>
<td><p>str</p></td>
<td><p>orientation and script as detected by tesseract</p></td>
</tr>
<tr class="row-even"><td><p>boxes</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">image_to_boxes</span></code></p></td>
<td><p>str</p></td>
<td><p>bounding boxes for each character</p></td>
</tr>
<tr class="row-odd"><td><p>data</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">image_to_data</span></code></p></td>
<td><p>str/tsv or df</p></td>
<td><p>tab separated table with boxes, confidences, line numbers</p></td>
</tr>
<tr class="row-even"><td><p>alto xml</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">image_to_alto_xml</span></code></p></td>
<td><p>str/xml</p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/ALTO_(XML)">ALTO XML</a> - standard for representing OCR and layout data in xml</p></td>
</tr>
<tr class="row-odd"><td><p>pdf</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">image_to_pdf_or_hocr</span></code></p></td>
<td><p>binary/pdf</p></td>
<td><p>Searchable PDF</p></td>
</tr>
<tr class="row-even"><td><p>hocr</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">image_to_pdf_or_hocr</span></code></p></td>
<td><p>str/hocr</p></td>
<td><p><a class="reference external" href="https://en.wikipedia.org/wiki/HOCR">hOCR</a> - Another standard for representing OCR data as valid html</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Each of these functions accept the <code class="docutils literal notranslate"><span class="pre">lang</span></code> and <code class="docutils literal notranslate"><span class="pre">config</span></code> parameters we have
already seen. Some have additional parameters, such as <code class="docutils literal notranslate"><span class="pre">image_to_data</span></code> which
accepts an <code class="docutils literal notranslate"><span class="pre">output_type</span></code> parameter which we will use later.</p>
<p>In an interactive python session, we can read documentation for functions with
the <code class="docutils literal notranslate"><span class="pre">help</span></code> function. See DataLab’s <a class="reference external" href="https://ucdavisdatalab.github.io/workshop_python_basics/chapters/01_python-basics.html#getting-help">introductory python reader
section</a>
for more information.  In this case, we can see the parameters for the
different pytesseract functions with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">help</span><span class="p">(</span><span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_alto_xml</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To save these outputs to disk we can use python <a class="reference external" href="https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files">file objects</a>.</p>
<p>For example when working with pdfs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pdfdata</span> <span class="o">=</span> <span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_pdf_or_hocr</span><span class="p">(</span><span class="s1">&#39;img.png&#39;</span><span class="p">,</span> <span class="n">extension</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;output.pdf&#39;</span><span class="p">,</span> <span class="s1">&#39;w+b&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">pdfdata</span><span class="p">)</span>
</pre></div>
</div>
<p>Another example saving the output in ALTO xml format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xml</span> <span class="o">=</span> <span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_alto_xml</span><span class="p">(</span><span class="s1">&#39;img.png&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;output.xml&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">xml</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="verbose-ocr-data">
<h4>Verbose OCR Data<a class="headerlink" href="#verbose-ocr-data" title="Permalink to this headline">¶</a></h4>
<p>Pytesseract’s <code class="docutils literal notranslate"><span class="pre">image_to_data</span></code> function provides word level data of the ocr
output. Parsing this information can be useful for many types of analyses. By
default, the return value is a string containing a table of tab seperated
values. However, when <a class="reference external" href="https://pandas.pydata.org/">pandas</a> is loaded, we can
have <code class="docutils literal notranslate"><span class="pre">image_to_data</span></code> return a <code class="docutils literal notranslate"><span class="pre">pandas</span></code> <strong>DataFrame</strong> object by setting the
<code class="docutils literal notranslate"><span class="pre">output_type</span></code> parameter:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pytesseract</span><span class="o">.</span><span class="n">image_to_data</span><span class="p">(</span><span class="n">toc_img</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">custom_oem_psm_config</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="s1">&#39;data.frame&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>level</th>
      <th>page_num</th>
      <th>block_num</th>
      <th>par_num</th>
      <th>line_num</th>
      <th>word_num</th>
      <th>left</th>
      <th>top</th>
      <th>width</th>
      <th>height</th>
      <th>conf</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2480</td>
      <td>3507</td>
      <td>-1.000000</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>376</td>
      <td>824</td>
      <td>444</td>
      <td>74</td>
      <td>-1.000000</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>376</td>
      <td>824</td>
      <td>444</td>
      <td>74</td>
      <td>-1.000000</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>376</td>
      <td>824</td>
      <td>444</td>
      <td>74</td>
      <td>-1.000000</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>376</td>
      <td>824</td>
      <td>444</td>
      <td>74</td>
      <td>96.769348</td>
      <td>Contents</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Pandas Dataframes are incredibly powerful for data analysis. For an introduction
to Pandas and dataframes see DataLab’s Python Basics <a class="reference external" href="https://ucdavisdatalab.github.io/workshop_python_basics/chapters/02_pandas-basics.html#dataframes">reader</a>.</p>
</div>
<p>Here is a summary of each column in this table. Adapted from <a class="reference external" href="https://blog.tomrochette.com/tesseract-tsv-format">this blog post</a></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Column</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>level</p></td>
<td><p>1: page, 2: block, 3: paragraph, 4: line, 5: word</p></td>
</tr>
<tr class="row-odd"><td><p>page_num</p></td>
<td><p>starts at 1, indicates page, only useful for multi-page documents</p></td>
</tr>
<tr class="row-even"><td><p>block_num</p></td>
<td><p>starts at 0, pages &gt; blocks &gt; paragraphs &gt; lines &gt; words</p></td>
</tr>
<tr class="row-odd"><td><p>par_num</p></td>
<td><p>starts at 0</p></td>
</tr>
<tr class="row-even"><td><p>line_num</p></td>
<td><p>starts at 0</p></td>
</tr>
<tr class="row-odd"><td><p>word_num</p></td>
<td><p>starts at 0</p></td>
</tr>
<tr class="row-even"><td><p>left</p></td>
<td><p>x coordinate in pixels, top left corner of the bounding box, from top left corner of image</p></td>
</tr>
<tr class="row-odd"><td><p>top</p></td>
<td><p>y coordinate in pixels, top left corner of the bounding box, from top left corner of image</p></td>
</tr>
<tr class="row-even"><td><p>width</p></td>
<td><p>width in pixels of bounding box</p></td>
</tr>
<tr class="row-odd"><td><p>height</p></td>
<td><p>height in pixels of bounding box</p></td>
</tr>
<tr class="row-even"><td><p>conf</p></td>
<td><p>confidence value for the word, 0-100, -1 for any row that isn’t a word</p></td>
</tr>
<tr class="row-odd"><td><p>text</p></td>
<td><p>detected word, NaN or empty for any row that isn’t a word</p></td>
</tr>
</tbody>
</table>
<p>A <strong>bounding box</strong> refers to a rectangular region within the image. Bounding
boxes can be used to represent a page, a block, a paragraph, a line, a word or
even a character.</p>
<p>Analysis of this data can be very useful for projects that rely on the <strong>layout</strong>
of the documents. One use of this data is to quickly classify types of pages
within your document set, for example, you could develop heuristics for
detecting if a page contains a table of contents and filter those out. You
could use this data for extracting Titles or Headers other sequences of text
that have differing text heights. Additionally, if you only care about the text
within a certain region of the page, for example the main article body of a
journal article, you could filter out the rows that aren’t within that region.</p>
<p>In addition to information about the layout, this table contains the
<strong>confidence values</strong> associated with each word of detected text. These scores
range from 0-100 and reflect the engine’s confidence in the detected word.</p>
</div>
</div>
</div>
<div class="section" id="assessing-accuracy">
<h2>Assessing Accuracy<a class="headerlink" href="#assessing-accuracy" title="Permalink to this headline">¶</a></h2>
<p>OCR is a very challenging problem, and while current tools are very advanced and built using the latest technologies, they are imperfect.</p>
<div class="section" id="confidence-scores">
<h3>Confidence scores<a class="headerlink" href="#confidence-scores" title="Permalink to this headline">¶</a></h3>
<p>One quantitative way of evaluating the OCR’s performance is by analyzing the <strong>confidence values</strong> returned from <code class="docutils literal notranslate"><span class="pre">pytesseract.image_to_data</span></code>. With Pandas we can compute some summary statistics on the values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;conf&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">notna</span><span class="p">()]</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>count    65.000000
mean     95.051439
std       2.124442
min      88.821213
25%      93.293381
50%      96.125114
75%      96.529099
max      96.911034
Name: conf, dtype: float64
</pre></div>
</div>
</div>
</div>
<p>We can also sort the words by their confidence scores</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;conf&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)[[</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;conf&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">notna</span><span class="p">()]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>conf</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>8</th>
      <td>8</td>
      <td>96.911034</td>
    </tr>
    <tr>
      <th>12</th>
      <td>9</td>
      <td>96.907730</td>
    </tr>
    <tr>
      <th>128</th>
      <td>67</td>
      <td>96.851425</td>
    </tr>
    <tr>
      <th>23</th>
      <td>of</td>
      <td>96.804993</td>
    </tr>
    <tr>
      <th>67</th>
      <td>Story</td>
      <td>96.780762</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>66</th>
      <td>Turtle’s</td>
      <td>91.321640</td>
    </tr>
    <tr>
      <th>55</th>
      <td>Tea-Party</td>
      <td>90.653679</td>
    </tr>
    <tr>
      <th>74</th>
      <td>Quadrille</td>
      <td>90.321297</td>
    </tr>
    <tr>
      <th>87</th>
      <td>Alice’s</td>
      <td>89.817886</td>
    </tr>
    <tr>
      <th>132</th>
      <td>73</td>
      <td>88.821213</td>
    </tr>
  </tbody>
</table>
<p>65 rows × 2 columns</p>
</div></div></div>
</div>
</div>
<div class="section" id="vocabulary">
<h3>Vocabulary<a class="headerlink" href="#vocabulary" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The               5
a                 3
11                2
A                 2
the               2
and               2
12                1
Tarts?            1
Stole             1
Contents          1
Who               1
Evidence          1
Quadrille         1
Lobster           1
10                1
Story             1
Alice’s           1
25                1
13                1
19                1
Mock              1
31                1
37                1
43                1
51                1
59                1
67                1
73                1
81                1
Turtle’s          1
Tea-Party         1
Croquet-Ground    1
Queen’s           1
9                 1
Down              1
Rabbit-Hole       1
Pool              1
of                1
Tears             1
Caucus-Race       1
Long              1
Tale              1
Rabbit            1
Sends             1
in                1
Little            1
Bill              1
Advice            1
from              1
Caterpillar       1
Pig               1
Pepper            1
Mad               1
8                 1
87                1
Name: text, dtype: int64
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="image-considerations">
<h2>Image Considerations<a class="headerlink" href="#image-considerations" title="Permalink to this headline">¶</a></h2>
<p>The quality of OCR outputs is heavily dependent on the quality of the input
image. There are many potentially problematic features that can lead to very
poor OCR results. Many of these problems can be programmatically resolved
before passing the image to the OCR engine through image preprocessing. There
are some instances where no amount of preprocessing will be sufficient to get
high quality OCR results. In order to maximize the quality of OCR results from
tesseract it is important to consider a few things. Many of these things are
quite intuitive if we consider how ‘tesseract’ is preforming the task of OCR.
So far we have worked with images that are very well suited to OCR. They are
well suited for several reasons: its easy to distinguish text from the
background, they are properly aligned, they contain nothing but text, they are
high quality (measured in dots per inch), and they contain standard fonts.</p>
<p>So what can we do with images that don’t look like the images we have seen so
far? It is a good idea to start by randomly selecting some of your images and
getting the OCR data. It is also good to consider what you are seeking to get
out of OCRing your images. Do you really need perfect quality transcription?
How many images are you hoping to process? How much manual intervention can you
apply?</p>
<p>Tesseract provides detailed
<a class="reference external" href="https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html">documentation</a> on
ways to improve accuracy. Many of these ways involve preprocessing the images.
You can do all of this with whatever graphical user interface image editor you
prefer. This may be your best option if you are working with relatively few
documents. If you are working with lots of documents, then you can develop a
preprocessing scheme with the graphical interface and work your way to
replicating the workflow programmatically using
<a class="reference external" href="https://imagemagick.org/index.php">imagemagick</a> or
<a class="reference external" href="https://opencv.org/">opencv</a>.</p>
<div class="section" id="working-with-pdfs">
<h3>Working with Pdfs<a class="headerlink" href="#working-with-pdfs" title="Permalink to this headline">¶</a></h3>
<p>Tesseract does not operate on Pdfs. To run OCR with tesseract on a Pdf, you
must first convert the pages of the pdf to an image file format.</p>
<div class="section" id="extracting-existing-text-layers">
<h4>extracting existing text layers<a class="headerlink" href="#extracting-existing-text-layers" title="Permalink to this headline">¶</a></h4>
<p>Often times Pdfs will have a text layer already embedded. In which case it may
not be required to run OCR. We can extract text layers, if they exist, with
<a class="reference external" href="https://pypdf2.readthedocs.io/en/latest/user/extract-text.html">PyPDF2</a> library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PyPDF2</span> <span class="kn">import</span> <span class="n">PdfFileReader</span>

<span class="n">reader</span> <span class="o">=</span> <span class="n">PdfFileReader</span><span class="p">(</span><span class="s1">&#39;./data/two_column-google.pdf&#39;</span><span class="p">)</span>
<span class="n">page</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">pages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">page</span><span class="o">.</span><span class="n">extractText</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [_reader.py:1065]
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;An Overview of the Tesseract OCR Engi ne \n \n \nRay Smith  \nGoogle Inc. \ntheraysmith@gmail.com \n \nAbst ra ct  \n \nThe Tesseract OCR engine, as was the HP Research \nPrototype in the UNLV Fourth Annual Test of OCR \n\nA ccuracy[1], is described in a comprehensive \n\noverview. Emphasis is placed on aspects that are novel \n\nor at least unusual in an OCR engine, including in \n\nparticular the line finding, features/classification \n\nmethods, and the adaptive classifier. \n  \n \n1. Introduction ŒMotivation and History \n \nTesseract is an open-source OCR engine that was \ndeveloped at HP between 1984 and 1994. Like a super-\n\nnova, it appear ed fr om nowher e for  the 1995 UNLV \n\nAnnual Test of OCR Accuracy [1], shone brightly with \n\nits results, and then vanished back under the same \n\ncloak of secrecy under which it had been developed. \n\nNow for the first time, deta ils of the architecture and \nalg orith ms can  be revealed . \nTesser act began as a PhD r esear ch pr oject [2] in HP \nLabs, Bristol, and gain ed momen tum as a possible \n\nsoftware and/or hardware add-on for HP™s line of \n\nflatbed scanners. Motivation was provided by the fact \n\nthat the commercial OCR engines of the day were in \n\ntheir infancy, and failed miserably on anything but the \n\nbest quality print. \nAfter a joint project between HP Labs Bristol, and \nHP™s scann er division  in  Colorado, Tesseract had a \n\nsignificant lea d in a ccura cy over the commercia l \n\nengines, but did not become a product. The next stage \nof its development was back in HP Labs Bristol as an \ninvestigation of OCR for compression. Work \n\nconcentrated more on improving rejection efficiency \n\nthan on base-level accuracy. At the end of this project, \n\nat the end of 1994, development ceased entirely. The \n\nengine was sent to UNLV for  the 1995 Annual Test of \n\nOCR Ac cura cy[1], where it proved its worth a gainst \n\nthe commercial engines of the time. In late 2005, HP \n\nreleased Tesseract for open source. It is now available \n\nat http://code.google.com/p/tesseract-ocr. \n\n \n2. Archi tecture \n \nSince HP had independently-developed page layout \nanalysis technology that was used in products, (and \n\ntherefore not released for open-source) Tesseract never \n\nneeded its own page layout analysis. Tesseract \n\ntherefore assumes that its input is a binary image with \n\noptional polygonal text regions defined. \nProcessing follows a traditional step-by-step \npipeline, but some of the stages wer e unusual in their \n\nda y, a nd possibly remain so even now. The first step is \n\na connected component analysis in which outlines of \n\nthe components are stored. This was a computationally \nexpensive design decision at the time, but had a \nsignificant advantage: by inspection of the nesting of \n\noutlines, and the number of child and grandchild \n\noutlines,  it is simple to detect inverse text and \n\nrecognize it as easily as black-on-white text. Tesseract \n\nwas probably the first OCR engine able to handle \n\nwhite-on-black text so trivially. At this stage, outlines \n\nare gathered together, purely by nesting,  into \nBlobs\n. \nBlobs are organized into text lines, and the lines and \nr egions ar e analyzed for  fixed pitch or  pr oportional \n\ntext. Text lines are broken into words differently \n\naccor ding to the kind of character spacing. Fixed pitch \ntext is chopped immediately by character cells. \nProportional text is broken into words using definite \n\nspaces and fuzzy spaces. \nRe c ognition then proceeds a s a two-pa ss process. In \nthe first pass, an attempt is made to recognize each \n\nword in turn. Each word that is satisfactory is passed to \n\nan ada ptive cla ssifier a s training data. The a da ptive  \n\nclassifier then gets a chance to more accurately \n\nrecognize text lower down the page. \nS ince the a da ptive cla ssifier ma y ha ve lea rned \nsomething useful too late to make a contribution near \nthe top of the page, a second pass is run over the page, \nin which words that were not recognized well enough \n\nare recognized again. \nA final phase resolves fuzzy spaces, and checks \nalternative hypotheses for the x-height to locate small-\n\ncap text. \n&#39;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="converting-pdfs-to-an-image-format">
<h4>converting pdfs to an image format<a class="headerlink" href="#converting-pdfs-to-an-image-format" title="Permalink to this headline">¶</a></h4>
<p>Using the <a class="reference external" href="https://github.com/Belval/pdf2image">pdf2image</a>
package we can convert pdfs to image formats:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pdf2image</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">pdf2image</span><span class="o">.</span><span class="n">convert_from_path</span><span class="p">(</span><span class="s1">&#39;./data/two_column-google.pdf&#39;</span><span class="p">,</span> <span class="n">output_folder</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;jpeg&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="text-cleaning">
<h2>Text Cleaning<a class="headerlink" href="#text-cleaning" title="Permalink to this headline">¶</a></h2>
<p>Depending on what analysis you want to do with your text, its helpful to know
some common text cleaning methods and ways of implementing them in python.
Preprocessing text for computational analysis is a huge topic that won’t be
covered here. You can read more about text processing in python in DataLab’s
<a class="reference external" href="https://ucdavisdatalab.github.io/workshop_getting_started_with_textual_data/">getting started with textual
data</a>
series. As well as the upcoming <a class="reference external" href="https://ucdavisdatalab.github.io/workshop_nlp_with_python/">Natural Language Processing
Series</a></p>
<p>Here is some sample code demonstrating common preprocessing techniques applied
to the output of the OCR data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">stop_words</span> <span class="kn">import</span> <span class="n">get_stop_words</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">notna</span><span class="p">()]</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;[^\w\s]&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;\d+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">stopwords</span> <span class="o">=</span> <span class="n">get_stop_words</span><span class="p">(</span><span class="s1">&#39;en&#39;</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>FutureWarning: The default value of regex will change from True to False in a future version. [1984863378.py:4]
FutureWarning: The default value of regex will change from True to False in a future version. [1984863378.py:5]
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="index.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Introduction</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="references.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">References</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Arthur Koehl<br/>
    
      <div class="extra_footer">
        <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">
  <img alt="CC BY-SA 4.0" src="https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg"> 
</a>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>